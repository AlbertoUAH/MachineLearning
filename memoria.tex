% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Práctica Machine Learning},
  pdfauthor={Fernández Hernández, Alberto},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage[font={small}]{caption}

\title{Práctica Machine Learning}
\author{Fernández Hernández, Alberto}
\date{4/28/2021}

\begin{document}
\maketitle

\begin{verbatim}
                                         [IIIII]
                                          )"""(
                                         /     \
                                        /       \
                                        |`-...-'|
                                        |aspirin|
                                      _ |`-...-'|    _
                                     (\)`-.___.(I) _(/)
                                       (I)  (/)(I)(\)
                                          (I)        
\end{verbatim}

\textbf{Nota}: a lo largo de la práctica, tanto en el tuneo de
hiperparámetros como en la comparación de modelos se ha llevado a cabo
mediante \textbf{validación cruzada repetida}, con la misma semilla
(1234) y nº de grupos (5).

\newpage

\hypertarget{introducciuxf3n-y-descripciuxf3n-de-los-datos}{%
\section{1. Introducción y descripción de los
datos}\label{introducciuxf3n-y-descripciuxf3n-de-los-datos}}

El objetivo del presente proyecto consiste en \textbf{elaborar un modelo
de clasificación binaria que permita predecir si un paciente presentará
o será más propenso a padecer una complicación hospitalaria tras una
intervención quirúrgica \footnote{\url{https://www.kaggle.com/omnamahshivai/surgical-dataset-binary-classification}}}.
Originalmente, el fichero (extraído de la plataforma Kaggle) contiene
tres variables objetivo, dos continuas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{ccsComplicationRate}: incidencia general de complicaciones
  hospitalarias por cada tipo de intervención quirúrgica.
\item
  \emph{complication\_rsi}: índice de estratificación de riesgo en
  complicaciones hospitalarias.
\end{enumerate}

Y una binaria:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \emph{complication}: \textbf{si el paciente ha sufrido una
  complicación (1) o no (0)}.
\end{enumerate}

Por tanto, de cara a la práctica tendremos únicamente en cuenta, como
variable objetivo, la columna \emph{complication}, descartando las dos
variables continuas anteriores.

En relación con las posibles variables \emph{input}, nos encontramos con
las siguientes:

\textbf{CONTINUAS}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{bmi}: \textbf{índice de masa corporal}.
\item
  \emph{Age}: \textbf{edad del paciente}.
\item
  \emph{baseline\_charlson}: \textbf{índice de comorbidad de Charlson,
  el cual predice la mortalidad a diez años de un paciente que puede
  tener una variedad de condiciones comórbidas (como una enfermedad
  cardíaca, SIDA o cáncer)}.
\item
  \emph{ahrq\_ccs}: \textbf{tipo de procedimiento/intervención
  quirúrgica, etiquetado por la Agencia estadounidense para la
  Investigación Sanitaria \footnote{\url{https://www.hcup-us.ahrq.gov/toolssoftware/ccs10/CCSCategoryNames(FullLabels).pdf}}}.
  Dicha variable contiene un total de 22 valores únicos, por lo que se
  ha decidido mantener la variable como numérica.
\item
  \emph{ccsMort30Rate}: \textbf{incidencia general de mortalidad a los
  30 días por cada intervención (dado por el código de la columna
  ahrq\_ccs)}.
\item
  \emph{hour}: \textbf{hora a la que se realizó la intervención}.
\item
  \emph{mortality\_rsi}: \textbf{índice de estratificación de riesgo en
  la mortalidad a los 30 días}.
\end{enumerate}

\textbf{CATEGÓRICAS}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  \emph{asa\_status}: \textbf{estado físico del paciente establecido por
  la Sociedad Americana de Anestesiología \footnote{\url{https://www.asahq.org/standards-and-guidelines/asa-physical-status-classification-system}}}.
  Contiene tres categorías:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  0: \textbf{estado I-II} (paciente sano / paciente con enfermedad
  sistémica leve).
\item
  1: \textbf{estado III} (paciente con enfermedad sistémica grave).
\item
  2: \textbf{estado IV-VI} (paciente con enfermedad muy grave / no
  espera sobrevivir sin la operación / muerte cerebral).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\item
  \emph{baseline\_cancer}: \textbf{¿El paciente padece algún cáncer?} (1
  = Sí; 0 = No)
\item
  \emph{baseline\_cvd}: \textbf{¿El paciente sufre alguna enfermedad
  cardio o cerebrovascular?} (1 = Sí; 0 = No)
\item
  \emph{baseline\_dementia}: \textbf{¿El paciente sufre algún trastorno
  por demencia?} (1 = Sí; 0 = No)
\item
  \emph{baseline\_diabetes}: \textbf{¿El paciente sufre diabetes?} (1 =
  Sí; 0 = No)
\item
  \emph{baseline\_digestive}: \textbf{¿El paciente sufre alguna
  enfermedad gastro-intestinal?} (1 = Sí; 0 = No)
\item
  \emph{baseline\_osteoart}: \textbf{¿El paciente padece
  osteoartritis\footnote{\url{https://dicciomed.usal.es/palabra/osteoartritis}}?}
  (1 = Sí; 0 = No)
\item
  \emph{baseline\_psych}: \textbf{¿El paciente padece algún desorden
  psiquiátrico?} (1 = Sí; 0 = No)
\item
  \emph{baseline\_pulmonar}: \textbf{¿El paciente sufre alguna
  enfermedad pulmonar?} (1 = Sí; 0 = No)
\item
  \emph{dow} o \emph{day of week}: \textbf{día de la semana en el que se
  realizó la intervención} (0 = Lunes; 1 = Martes; 2 = Miércoles; 3 =
  Jueves; 4 = Viernes).
\item
  \emph{month}: \textbf{mes en el que se realizó la intervención}.
\item
  \emph{moonphase}: \textbf{fase lunar que tuvo lugar durante la
  intervención quirúrgica} (0 = Luna nueva; 1 = Cuarto creciente; 2 =
  Luna llena; 3 = Cuarto menguante).
\item
  \emph{mort30}: \textbf{¿El paciente presenta algún riesgo de fallecer
  a los 30 días?} (1 = Sí; 0 = No)
\item
  \emph{gender}: \textbf{Sexo del paciente} (0 = Hombre; 1 = Mujer)
\item
  \emph{race}: \textbf{raza del paciente} (0 = Caucásico; 1 =
  Afroameriano; 2 = Otro)
\end{enumerate}

\hypertarget{libreruxedas-empleadas}{%
\section{2. Librerías empleadas}\label{libreruxedas-empleadas}}

A continuación, se expone un listado de las librerías empleadas en el
desarrollo del proyecto:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{caret}: tuneo de hipérparámetros de los diferentes algoritmos de
  clasificación.
\item
  \emph{data.table}: estructura de datos, similar al \emph{data.frame},
  aunque mucho más eficiente en memoria.
\item
  \emph{ggplot2}: librería gráfica.
\item
  \emph{scorecard}: cálculo del valor de información (IV), así como el
  peso de la evidencia (WOE).
\item
  \emph{dummies}: transformación de variables categóricas a
  \emph{dummies}.
\item
  \emph{forcats}: tratamiento de variables categóricas.
\item
  \emph{inspectdf}: libreria para inspeccionar las caracteristicas
  principales de un \emph{dataset}, incluyendo variables categóricas,
  valores \emph{missing} o distribución de las variables continuas.
\item
  \emph{dplyr}: manipulación de datos.
\item
  \emph{psych}: información general de data.frames y/o data.tables
  (media, asimetría, desviación típica, entre otros).
\item
  \emph{doParallel} y \emph{parallel}: paralelización de funciones.
\item
  \emph{readxl}: lectura de ficheros \emph{Excel} (.xlsx).
\item
  \emph{purrr}: herramientas de programación funcional.
\item
  \emph{visualpred}: visualización de predicciones por diferentes
  algoritmos de clasificación.
\item
  \emph{h2o}: \emph{auto Machine Learning (autoML)}.
\item
  \textbf{Librerías y funciones proporcionadas por el profesor}.
\end{enumerate}

\hypertarget{depuraciuxf3n-de-los-datos}{%
\section{3. Depuración de los datos}\label{depuraciuxf3n-de-los-datos}}

Inicialmente, comenzamos con la lectura del fichero:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Lectura del fichero}
\NormalTok{surgical\_dataset <{-}}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\StringTok{"./data/Surgical{-}deepnet.csv"}\NormalTok{, }\DataTypeTok{data.table =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{\# Eliminamos las dos variables objetivo continuas}
\NormalTok{surgical\_dataset}\OperatorTok{$}\NormalTok{ccsComplicationRate <{-}}\StringTok{ }\OtherTok{NULL}\NormalTok{; surgical\_dataset}\OperatorTok{$}\NormalTok{complication\_rsi <{-}}\StringTok{ }\OtherTok{NULL}

\KeywordTok{dim}\NormalTok{(surgical\_dataset) }\CommentTok{\# Filas x columnas}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14635    23
\end{verbatim}

Nos encontramos con 14.635 observaciones, junto con las 23 variables
descritas anteriormente. Si echamos un vistazo a la variable objetivo,
podemos observar el desbalanceo entre ambas categorías:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(surgical\_dataset}\OperatorTok{$}\NormalTok{complication)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##     0     1 
## 10945  3690
\end{verbatim}

\hypertarget{codificaciuxf3n-a-factor}{%
\subsection{\texorpdfstring{3.1 Codificación a
\emph{factor}}{3.1 Codificación a factor}}\label{codificaciuxf3n-a-factor}}

Tras la lectura del fichero, \textbf{codificamos como \emph{factor}
tanto la variable objetivo como el resto de variables categóricas}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Codificamos como factor la variable objetivo...}
\NormalTok{surgical\_dataset}\OperatorTok{$}\NormalTok{complication <{-}}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(surgical\_dataset}\OperatorTok{$}\NormalTok{complication)}
\CommentTok{\# ...Asi como el resto de variables categoricas mencionadas anteriormente}
\NormalTok{cat\_columns <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"gender"}\NormalTok{, }\StringTok{"race"}\NormalTok{, }\StringTok{"asa\_status"}\NormalTok{, }\StringTok{"baseline\_cancer"}\NormalTok{, }\StringTok{"baseline\_cvd"}\NormalTok{, }
                 \StringTok{"baseline\_dementia"}\NormalTok{, }\StringTok{"baseline\_diabetes"}\NormalTok{, }\StringTok{"baseline\_digestive"}\NormalTok{, }
                 \StringTok{"baseline\_osteoart"}\NormalTok{, }\StringTok{"baseline\_psych"}\NormalTok{, }\StringTok{"baseline\_pulmonary"}\NormalTok{, }
                 \StringTok{"dow"}\NormalTok{, }\StringTok{"month"}\NormalTok{, }\StringTok{"moonphase"}\NormalTok{, }\StringTok{"mort30"}\NormalTok{)}
\NormalTok{surgical\_dataset[,cat\_columns] <{-}}\StringTok{ }\KeywordTok{lapply}\NormalTok{(surgical\_dataset[, cat\_columns], factor)}
\end{Highlighting}
\end{Shaded}

A continuación, almacenamos los nombres de cada variable en un vector
por separado, \textbf{en función de si es continua o categórica}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Separamos las variables en numericas, categoricas y target}
\CommentTok{\# [{-}16] => Salvo la variable objetivo}
\NormalTok{cat\_columns <{-}}\StringTok{ }\KeywordTok{names}\NormalTok{(}\KeywordTok{Filter}\NormalTok{(is.factor, surgical\_dataset))[}\OperatorTok{{-}}\DecValTok{16}\NormalTok{]}
\NormalTok{num\_columns <{-}}\StringTok{ }\KeywordTok{names}\NormalTok{(}\KeywordTok{Filter}\NormalTok{(is.numeric, surgical\_dataset))}
\NormalTok{target      <{-}}\StringTok{ "complication"}
\end{Highlighting}
\end{Shaded}

\hypertarget{valores-na}{%
\subsection{3.2 Valores NA}\label{valores-na}}

Como se puede comprobar a continuación, el \emph{dataset} \textbf{no
contiene valores \emph{missing} en ninguna de las variables}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(surgical\_dataset))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\hypertarget{variables-categuxf3ricas}{%
\subsection{3.3 Variables categóricas}\label{variables-categuxf3ricas}}

Tras almacenar los nombres de cada variable, mediante la librería
\emph{inspectdf} se realizó un primer análisis exploratorio de datos
automático con el que \textbf{analizar el dataset en primera instancia}.
Dado que el contenido del informe es muy extenso, se incluirá en la
memoria el contenido esencial (el informe completo se incluye,
desglosado, en los anexos \emph{00\_EDA\_report.pdf} y
\emph{WOEBIN\_factor\_variables.pdf} ).

Sobre dicho informe, \textbf{comenzamos remarcando la frecuencia de
aparición de los niveles de cada variable categórica}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <{-}}\StringTok{ }\NormalTok{inspectdf}\OperatorTok{::}\KeywordTok{inspect\_cat}\NormalTok{(surgical\_dataset[, cat\_columns], }\DataTypeTok{include\_int =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{show\_plot}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\includegraphics{memoria_files/figure-latex/unnamed-chunk-7-1}

A simple vista, prácticamente todas las categorías presentan una alta
frecuencia de aparición, \textbf{salvo por \emph{baseline\_dementia} y
\emph{mort30}}, donde el número de observaciones a 1 es de 71 y 58,
respectivamente.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surgical\_dataset[, }\KeywordTok{c}\NormalTok{(}\StringTok{"baseline\_dementia"}\NormalTok{, }\StringTok{"mort30"}\NormalTok{)] }\OperatorTok{\%>\%}\StringTok{ }\KeywordTok{map}\NormalTok{(table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $baseline_dementia     $mort30
##
##     0     1                0       1
## 14564    71            14577      58
\end{verbatim}

Es decir, se tratan de variables con pocas observaciones con valor 1. De
hecho, si analizamos el valor de información haciendo uso del paquete
\emph{scorecard}, la cual nos permite estudiar el ``poder predictivo de
una variable'', observamos que el valor de información es cero,
\textbf{dada la poca representatividad de los valores a 1}, de forma que
el paquete \emph{scorecard} acaba uniendo ambas categorías, lo que se
traduce en un escaso poder de predicción:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./capturas/Depuracion/woebin_mort30_dementia} 

}

\caption{Mort 30 y Baseline dementia (IV)}\label{fig:unnamed-chunk-9}
\end{figure}

Por otro lado, si analizamos la proporción de aparición de la variable
objetivo sobre cada categoría:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} baseline\_dementia}
\NormalTok{surgical\_dataset }\OperatorTok{\%>\%}
\StringTok{    }\KeywordTok{count}\NormalTok{(baseline\_dementia, complication) }\OperatorTok{\%>\%}
\StringTok{    }\KeywordTok{group\_by}\NormalTok{(complication)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 3
## # Groups:   complication [2]
##   baseline_dementia complication     n
##   <fct>             <fct>        <int>
## 1 0                 0            10913
## 2 0                 1             3651
## 3 1                 0               32
## 4 1                 1               39
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} mort30}
\NormalTok{surgical\_dataset }\OperatorTok{\%>\%}
\StringTok{    }\KeywordTok{count}\NormalTok{(mort30, complication) }\OperatorTok{\%>\%}
\StringTok{    }\KeywordTok{group\_by}\NormalTok{(complication)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 3
## # Groups:   complication [2]
##   mort30 complication     n
##   <fct>  <fct>        <int>
## 1 0      0            10924
## 2 0      1             3653
## 3 1      0               21
## 4 1      1               37
\end{verbatim}

A simple vista, en ambas variables \textbf{no existe una clara
separación sobre la variable objetivo}. Por tanto, se ha tomado la
decisión de descartar ambas columnas del conjunto de datos.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surgical\_dataset}\OperatorTok{$}\NormalTok{baseline\_dementia <{-}}\StringTok{ }\OtherTok{NULL}\NormalTok{; surgical\_dataset}\OperatorTok{$}\NormalTok{mort30 <{-}}\StringTok{ }\OtherTok{NULL} 

\CommentTok{\# Actualizamos el vector con las variables categoricas}
\NormalTok{cat\_columns <{-}}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(cat\_columns, }\KeywordTok{c}\NormalTok{(}\StringTok{"baseline\_dementia"}\NormalTok{, }\StringTok{"mort30"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{agrupaciuxf3n-de-variables-categuxf3ricas}{%
\subsubsection{3.3.1 Agrupación de variables
categóricas}\label{agrupaciuxf3n-de-variables-categuxf3ricas}}

Por otro lado, nos encontramos con dos variables cuyas categorías pueden
ser agrupadas, según la información proporcionada por el paquete
\emph{scorecard}:

\textbf{DÍA DE LA SEMANA} (dow):

Sobre dicha variable, \textbf{observamos una relación ``lineal'' en la
distribución de la variable objetivo a lo largo de los diferentes días
de la semana}, comenzando por el Lunes (0), con el menor porcentaje de
complicaciones hospitalarias (alrededor del 14 \%), seguido de los
Martes-Miércoles-Jueves, donde el porcentaje aumenta hasta el 30.4 \%, y
finalizando con los viernes, donde se alcanza el mayor porcentaje de
complicaciones hospitalarias: 34.5 \%:

\begin{figure}[h!]

{\centering \includegraphics[width=0.65\linewidth,height=0.65\textheight,]{./capturas/Depuracion/dow} 

}

\caption{Dia de la semana o dow (IV)}\label{fig:unnamed-chunk-12}
\end{figure}

Por otro lado, si analizamos detenidamente el gráfico de distribución:

\includegraphics{memoria_files/figure-latex/unnamed-chunk-13-1}

Observamos que la proporción de aparición de pacientes con
complicaciones es muy similar entre los martes, miércoles y jueves:

\begin{verbatim}
##                   dow sin.comp con.comp total prop.complicacion
## 1                   1     4450      715  2596              30.3
## 2                   2     1810      786  2029              33.6
## 3                   3     1347      682  2516              27.9
## 4 En conjunto (1-2-3)     1813      703  7141              30.4
## 5                   4     1525      804  2329              34.5
\end{verbatim}

En conjunto, acumulan alrededor del 30.4 \% de pacientes con
complicaciones, mientras que con tan solo el viernes aumenta hasta
alcanzar el 34 \%. Por tanto, dado que los martes, miércoles y jueves
presentan una proporción de aparición similar, \textbf{las agrupamos en
torno a una misma categoría}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Lunes (0)}
\item
  \textbf{Martes-Miercoles-Jueves (1-3)}
\item
  \textbf{Viernes (4)}
\end{enumerate}

\textbf{MES} (month):

\begin{figure}[h!]

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight,]{./capturas/Depuracion/month} 

}

\caption{Month (IV)}\label{fig:unnamed-chunk-15}
\end{figure}

En este caso, llaman la atención tres principales grupos: en primer
lugar el mes de enero (0), con un 28.1 \% de las complicaciones
hospitalarias, \textbf{seguido de los meses de febrero (1) hasta agosto
(7) con un total acumulado del 32.3 \% de los pacientes con
complicaciones}, es decir, el mes de enero tiene un porcentaje similar
de pacientes con complicaciones que los siguientes 7 meses en conjunto.
Por el contrario, \textbf{durante el mes de septiembre (8) el porcentaje
se desploma hasta el 7.8 \%}, porcentaje que vuelve a aumentar en los
tres meses siguientes (octubre, noviembre y diciembre), hasta el 31.4
\%.

Por otro lado, si analizamos el gráfico de distribución:

\includegraphics{memoria_files/figure-latex/unnamed-chunk-16-1}

Sucede un comportamiento similar al de la variable \emph{dow}: salvo el
mes de septiembre, \textbf{la distribución de la variable objetivo sobre
cada mes es muy similar, de forma que podemos agrupar varios de los
meses en una misma categoría, tal y como hemos comprobado
anteriormente}. A modo de ejemplo, analicemos la proporción en el número
de complicaciones hospitalarias de cada mes por separado y en conjunto:

\begin{verbatim}
##             dow sin.comp con.comp total prop.complicacion
## 1             0      771      302  1073              28.1
## 2             1      742      385  1127              34.2
## 3             2      461      220   681              32.3
## 4             3      670      319   989              32.3
## 5             4      699      382  1081              35.3
## 6             5      618      271   889              30.5
## 7             6      810      381  1191              32.0
## 8             7      739      301  1040              28.9
## 9   Uniendo 1-7     4739     2259  6998              32.3
## 10            8     3634      306  3940               7.8
## 11            9      586      246   832              29.6
## 12           10      541      258   799              32.3
## 13           11      674      319   993              32.1
## 14 Uniendo 9-11     1801      823  2624              31.4
\end{verbatim}

Como podemos comprobar, \textbf{la proporción de pacientes con
complicaciones hospitalarias, entre los meses de febrero (1) y agosto
(7), se sitúa en torno al 32 \% si agrupamos dichas categorías}. Del
mismo modo, \textbf{los meses de octubre, noviembre y diciembre (9, 10 y
11) se sitúan en torno al 31 \%}. Como consecuencia, y dado que dichas
categorías presentan una proporción similar en cuanto a pacientes con
complicaciones se refiere, las agrupamos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Enero (0)}
\item
  \textbf{Febrero a Agosto (1-7)}
\item
  \textbf{Septiembre (8)}
\item
  \textbf{Octubre, Noviembre y Diciembre (9-10-11)}
\end{enumerate}

En relación con el resto de variables categóricas, si analizamos su
valor de información:

\begin{verbatim}
##              variable     iv
## 1   baseline_osteoart 0.5246
## 2               month 0.4271
## 3                 dow 0.2276
## 4           moonphase 0.2119
## 5     baseline_cancer 0.1358
## 6        baseline_cvd 0.0429
## 7              gender 0.0221
## 8  baseline_digestive 0.0134
## 9          asa_status 0.0062
## 10 baseline_pulmonary 0.0053
## 11  baseline_diabetes 0.0013
## 12               race 0.0002
## 13     baseline_psych 0.0001
\end{verbatim}

Por lo general, la mayoría de las variables categóricas presentan, como
primera impresión, un buen poder predicitivo, con ciertas excepciones
como \emph{asa\_status}, \emph{baseline\_pulmonary},
\emph{baseline\_diabetes}, \emph{race} o \emph{baseline\_pysch}, cuyo IV
no alcanza el 0.01 (como normal general, un valor de información
inferior a 0.1-0.02 se traduce en un poder predictivo muy bajo o
prácticamente nulo)\footnote{\url{https://docs.tibco.com/pub/sfire-dsc/6.5.0/doc/html/TIB_sfire-dsc_user-guide/GUID-07A78308-525A-406F-8221-9281F4E9D7CF.html}}.
De este modo, de cara a la selección de variables, \textbf{en caso de
ser necesario descartar alguna variable categórica, utilizaremos esta
tabla como referencia}.

\hypertarget{variables-continuas}{%
\subsection{3.4 Variables continuas}\label{variables-continuas}}

En relación con las variables continuas, nos encontramos con algunas
variables con un número de valores únicos muy reducidos:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} Nº Valores unicos (continuas)}
\KeywordTok{apply}\NormalTok{(surgical\_dataset[, num\_columns], }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) \{}\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(x))\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               bmi               Age baseline_charlson          ahrq_ccs 
##              3095               672                14                22 
##     ccsMort30Rate              hour     mortality_rsi 
##                20               725               633
\end{verbatim}

A modo de ejemplo, variables como \emph{baseline\_charlson},
\emph{ahrq\_ccs} o \emph{ccsMort30Rate} presentan 14, 22 o 20 valores
únicos, respectivamente. No obstante, se tomó la decisión de mantener
dichas variables como continuas, principalmente para facilitar la
selección de variables, evitando con ello crear nuevas variables
\emph{dummy} (una por cada categoría), utilizando con ello menos
parámetros para describir el conjunto de datos.

Por otro lado, en un primer análisis exploratorio se observó que en las
variables continuas, por lo general, \textbf{no existe una clara
separación entre ambas variables objetivo}. A modo de ejemplo,
observemos las dos siguientes variables: \emph{age} y \emph{bmi}:

\textbf{AGE}:

\includegraphics{memoria_files/figure-latex/unnamed-chunk-21-1}

En este caso, \textbf{bien es cierto que los pacientes que no sufren
complicaciones oscilan entre los 25 y los 90 años, aproximadamente,
mientras que existe un número de pacientes (menores a 25 años), donde
padecen alguna complicación hospitalaria}. Pese a ello, la separación no
es tan clara.

\textbf{BMI}:

\includegraphics{memoria_files/figure-latex/unnamed-chunk-22-1}

Incluso con el índice de masa de corporal, \textbf{a simple vista no
existe una clara separación que diferencie a ambas variables objetivo en
función de dicho índice}.

\hypertarget{estandarizaciuxf3n-de-variables-continuas}{%
\subsection{3.5 Estandarización de variables
continuas}\label{estandarizaciuxf3n-de-variables-continuas}}

Una vez realizado el primer análisis, \textbf{procedemos a la
estandarización de las variables continuas}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# {-}{-}{-} Estandarizacion de variables}
\NormalTok{surgical\_dataset\_stnd <{-}}\StringTok{ }\NormalTok{surgical\_dataset}

\CommentTok{\# media}
\NormalTok{media       <{-}}\StringTok{ }\KeywordTok{sapply}\NormalTok{(surgical\_dataset\_stnd[, num\_columns], mean)}

\CommentTok{\# sd}
\NormalTok{desv.tipica <{-}}\StringTok{ }\KeywordTok{sapply}\NormalTok{(surgical\_dataset\_stnd[, num\_columns], sd)}

\NormalTok{surgical\_dataset\_stnd[, num\_columns] <{-}}\StringTok{ }\KeywordTok{scale}\NormalTok{(surgical\_dataset\_stnd[, num\_columns], }
                                              \DataTypeTok{center =}\NormalTok{ media, }
                                              \DataTypeTok{scale =}\NormalTok{ desv.tipica)}
\end{Highlighting}
\end{Shaded}

\hypertarget{creacion-de-variables-dummy}{%
\subsection{3.6 Creacion de variables
dummy}\label{creacion-de-variables-dummy}}

A continuación, convertimos las variables categóricas (con más de una
categoría) en variables \emph{dummy}, mediante la función
\emph{dummy.data.frame}, conviritiendo de este modo todas las variables
a formato numérico:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{columnas\_dummy <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"asa\_status"}\NormalTok{, }\StringTok{"dow"}\NormalTok{, }\StringTok{"month"}\NormalTok{, }\StringTok{"moonphase"}\NormalTok{, }\StringTok{"race"}\NormalTok{)}
\NormalTok{surgical\_dataset\_stnd\_dummy <{-}}\StringTok{ }\KeywordTok{dummy.data.frame}\NormalTok{(surgical\_dataset\_stnd[, columnas\_dummy], }
                                                            \DataTypeTok{sep =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Una vez codificadas las variables, \textbf{comprobamos si existen
variables \emph{dummy} con una frecuencia de aparición menor a 100}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(surgical\_dataset\_stnd\_dummy[, }\KeywordTok{colSums}\NormalTok{(surgical\_dataset\_stnd\_dummy }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{<}\StringTok{ }\DecValTok{100}\NormalTok{, }
                                  \DataTypeTok{drop =} \OtherTok{FALSE}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## character(0)
\end{verbatim}

Como podemos comprobar, todas las variables \emph{dummy} codificadas
presentan una frecuencia superior. A continuación, \textbf{unimos en un
único \emph{data.table} tanto las variables numéricas, las variables
categóricas (binarias), las variables \emph{dummy}, así como la variable
objetivo}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surgical\_dataset\_final <{-}}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}
\NormalTok{                                surgical\_dataset\_stnd[, num\_columns],}
\NormalTok{                                surgical\_dataset\_stnd[, cat\_columns[}\OperatorTok{!}\NormalTok{cat\_columns }\OperatorTok{\%in\%}\StringTok{ }\NormalTok{columnas\_dummy]],}
\NormalTok{                                surgical\_dataset\_stnd\_dummy,}
\NormalTok{                                surgical\_dataset\_stnd[, target]}
\NormalTok{                              )}
\end{Highlighting}
\end{Shaded}

Finalmente, de cara a la elaboración de los modelos \textbf{codificamos
la variable objetivo como ``Yes'' / ``No''}, renombrando la variable
como \emph{target}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} Renombramos la variable objetivo como "target"}
\KeywordTok{names}\NormalTok{(surgical\_dataset\_final)[}\DecValTok{33}\NormalTok{] <{-}}\StringTok{ "target"}

\CommentTok{\# Renombramos las columnas para adecuarlas a formulas}
\KeywordTok{names}\NormalTok{(surgical\_dataset\_final) <{-}}\StringTok{ }\KeywordTok{make.names}\NormalTok{(}\KeywordTok{names}\NormalTok{(surgical\_dataset\_final))}

\CommentTok{\# 1 {-} "Yes" ; 0 {-} "No"}
\NormalTok{surgical\_dataset\_final}\OperatorTok{$}\NormalTok{target <{-}}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}
\NormalTok{  surgical\_dataset\_final}\OperatorTok{$}\NormalTok{target }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{,}
  \StringTok{"Yes"}\NormalTok{,}
  \StringTok{"No"}
\NormalTok{)}

\NormalTok{surgical\_dataset\_final}\OperatorTok{$}\NormalTok{target <{-}}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(surgical\_dataset\_final}\OperatorTok{$}\NormalTok{target)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Numero de variables finales (dummies incluidas):  32
\end{verbatim}

\hypertarget{selecciuxf3n-de-variables-bajo-loguxedstica}{%
\section{4. Selección de variables bajo
logística}\label{selecciuxf3n-de-variables-bajo-loguxedstica}}

\hypertarget{selecciuxf3n-de-una-submuestra}{%
\subsection{4.1 Selección de una
submuestra}\label{selecciuxf3n-de-una-submuestra}}

Durante la selección de variables, e incluso con determinados modelos
como redes neuronales o SVM, el tiempo de cómputo que requiere era
demasiado elevado. Como consecuencia, se planteó utilizar, en lugar del
\emph{dataset} completo con 14.000 observaciones, \textbf{un subconjunto
de menor tamaño}. Dado que la variable objetivo se encuentra
desbalanceada, como pudimos comprobar en el apartado de Depuración,
recurrimos al \textbf{muestreo estratificado}, el cual nos asegura la
proporción de la variable objetivo. Para ello, \emph{caret} dispone de
la función \emph{createDataPartition}, con el que obtener una submuestra
estratificada. Concretamente, para el desarrollo del proyecto empleamos
un subconjunto con el \textbf{40 \% de las observaciones}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} Muestreo estratificado}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{partitions <{-}}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(surgical\_dataset\_final}\OperatorTok{$}\NormalTok{target, }\DataTypeTok{p =} \FloatTok{0.40}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}

\NormalTok{surgical\_dataset\_final\_est <{-}}\StringTok{ }\NormalTok{surgical\_dataset\_final[partitions, ]}

\CommentTok{\# Mantenemos la misma proporcion en la variable objetivo}
\KeywordTok{table}\NormalTok{(surgical\_dataset\_final\_est}\OperatorTok{$}\NormalTok{target)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   No  Yes 
## 4378 1476
\end{verbatim}

No obstante, con un 40 \% de los datos ¿Es suficiente? Más importante
aún ¿Se obtienen los mismos resultados o similares? Es decir, si nos
fijamos en la depuración final, nos encontramos con 14.635 / 32
\textasciitilde{} 457 observaciones por variable, aproximadamente. Sin
embargo, con tan solo el 40 \%, obtendríamos (14.635 * 0.4) / 32
\textasciitilde{} 183 observaciones por variable. Es decir, con una
menor proporción de observaciones por variable, es posible que los
modelos no sean exactamente iguales con ambos conjuntos.

Por ello, durante la selección de variables \textbf{aplicaremos las
mismas técnicas sobre ambos conjuntos}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{stepwise AIC}
\item
  \emph{stepwise BIC}
\item
  \emph{RFE o \emph{Recursive Feature Elimination} (sobre logística)}
\item
  \emph{RFE (sobre \emph{Random Forest})}
\end{enumerate}

A continuación, y en caso de obtener una selección de variables similar,
aplicamos un modelo logístico en ambos casos, comprobando de este modo
si los resultados son similares, incluso con tan solo el 40 \% de las
observaciones. De ser así, de cara al resto de modelos trabajaremos con
el subconjunto.

\hypertarget{stepwise-aic}{%
\subsection{4.2 Stepwise AIC}\label{stepwise-aic}}

Comenzamos con la selección de variables bajo \emph{stepwise AIC}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} Stepwise AIC (100 repeticiones)}
\NormalTok{lista.variables.aic <{-}}\StringTok{ }\KeywordTok{steprepetidobinaria}\NormalTok{(}\DataTypeTok{data=}\NormalTok{surgical\_dataset,}
                                           \DataTypeTok{vardep=}\NormalTok{target,}\DataTypeTok{listconti=}\NormalTok{vars,}
                                           \DataTypeTok{sinicio=}\DecValTok{1234}\NormalTok{,}\DataTypeTok{sfinal=}\DecValTok{1334}\NormalTok{,}
                                           \DataTypeTok{porcen=}\FloatTok{0.8}\NormalTok{,}\DataTypeTok{criterio=}\StringTok{"AIC"}\NormalTok{)}
\NormalTok{tabla.aic <{-}}\StringTok{ }\NormalTok{lista.variables.aic[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     n varible_dataset_original   variable_subset
## 1   1            mortality_rsi     mortality_rsi
## 2   2            ccsMort30Rate     ccsMort30Rate
## 3   3                      bmi               bmi
## 4   4                  month.8           month.8
## 5   5             baseline_cvd      baseline_cvd
## 6   6                    dow.0             dow.0
## 7   7                      Age               Age
## 8   8              moonphase.0       moonphase.0
## 9   9                  month.0           month.0
## 10 10             asa_status.0      asa_status.0
## 11 11        baseline_osteoart baseline_osteoart
## 12 12        baseline_charlson baseline_charlson
## 13 13                 ahrq_ccs          ahrq_ccs
## 14 14        baseline_diabetes baseline_diabetes
## 15 15          baseline_cancer                 -
## 16 16           baseline_psych                 -
## 17 17             asa_status.1                 -
\end{verbatim}

Como podemos observar, \textbf{la diferencia entre el \emph{dataset}
original y la submuestra es de tan solo 3 variables:
\emph{baseline\_cancer}, \emph{baseline\_psych} y \emph{asa\_status.1}}.

\hypertarget{stepwise-bic}{%
\subsection{4.3 Stepwise BIC}\label{stepwise-bic}}

Por otro lado, analicemos la selección de variables bajo \emph{stepwise
BIC}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} Stepwise BIC (100 repeticiones)}
\NormalTok{lista.variables.bic <{-}}\StringTok{ }\KeywordTok{steprepetidobinaria}\NormalTok{(}\DataTypeTok{data=}\NormalTok{surgical\_dataset,}
                                           \DataTypeTok{vardep=}\NormalTok{target,}\DataTypeTok{listconti=}\NormalTok{vars,}
                                           \DataTypeTok{sinicio=}\DecValTok{1234}\NormalTok{,}\DataTypeTok{sfinal=}\DecValTok{1334}\NormalTok{,}
                                           \DataTypeTok{porcen=}\FloatTok{0.8}\NormalTok{,}\DataTypeTok{criterio=}\StringTok{"BIC"}\NormalTok{)}
\NormalTok{tabla.bic <{-}}\StringTok{ }\NormalTok{lista.variables.bic[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     n varible_dataset_original   variable_subset
## 1   1            mortality_rsi     mortality_rsi
## 2   2            ccsMort30Rate     ccsMort30Rate
## 3   3                      bmi               bmi
## 4   4                  month.8           month.8
## 5   5                    dow.0             dow.0
## 6   6                      Age               Age
## 7   7              moonphase.0       moonphase.0
## 8   8        baseline_osteoart baseline_osteoart
## 9   9             asa_status.0      asa_status.0
## 10 10          baseline_cancer                 -
\end{verbatim}

En este caso, la diferencia en ambas seleciones es de tan solo una única
variable: \emph{baseline\_cancer}.

\hypertarget{recursive-feature-elimination-bajo-loguxedstica}{%
\subsection{4.4 Recursive Feature Elimination (bajo
logística)}\label{recursive-feature-elimination-bajo-loguxedstica}}

No obstante, en ambas seleccio fnes de variables el número de parámetros
es bastante elevado, en especial con \emph{stepwise AIC}. Como
consecuencia, a los métodos anteriores añadimos una selección de
variables mediante \emph{Recursive Feature Elimination}, tanto bajo
logística como con \emph{Random Forest}:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/01_feature_selection_RFE_LR_whole_dataset} 

}

\caption{RFE Logistic Regression}\label{fig:unnamed-chunk-34}
\end{figure}

Analizando el gráfico resultante, observamos que la diferencia de
\emph{Accuracy} en ambos \emph{datasets} no es muy significativa:
\textbf{mientras que con el \emph{dataset} completo se alcanza un valor
en torno a 0.78, con el subconjunto la precisión se reduce a tan solo
0.77}. De hecho, en ambos casos se obtienen muy buenos resultados con
tan solo tres variables, y en ambos casos coinciden:

\begin{verbatim}
## Top 3 variables:

## [1] ccsMort30Rate
## [2] mortality_rsi
## [3] bmi
\end{verbatim}

\hypertarget{recursive-feature-elimination-bajo-random-forest}{%
\subsection{4.5 Recursive Feature Elimination (bajo Random
Forest)}\label{recursive-feature-elimination-bajo-random-forest}}

En el caso de \emph{Random Forest}, \textbf{con 4-5 parámetros se
obtienen resultados muy similares}: en el caso el \emph{dataset}
original en torno a 0.90 y 0.88 el caso del subconjunto. De hecho, en el
punto de máximo \emph{accuracy}, ambos modelos comparten las mismas
variables, a excepción de \emph{ahrq\_ccs}:

\begin{verbatim}
##   n varible_dataset_original variable_subset
## 1 1                      Age             Age
## 2 2            ccsMort30Rate   ccsMort30Rate
## 3 3            mortality_rsi   mortality_rsi
## 4 4                      bmi             bmi
## 5 5                        -        ahrq_ccs
\end{verbatim}

\newpage
\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/01_feature_selection_RFE_RF_whole_dataset} 

}

\caption{RFE Random Forest}\label{fig:unnamed-chunk-36}
\end{figure}

\hypertarget{selecciuxf3n-bajo-loguxedstica}{%
\subsection{4.6 Selección bajo
logística}\label{selecciuxf3n-bajo-loguxedstica}}

\hypertarget{comparaciuxf3n-datasets}{%
\subsubsection{4.6.1 Comparación
datasets}\label{comparaciuxf3n-datasets}}

A continuación, y una vez realizada la selección de variables tanto por
AIC/BIC como por RFE, \textbf{realizamos una primera comparación, bajo
logística, de la selección de variables obtenidas en los métodos
anteriores, tanto con el \emph{dataset} original como con el
subconjunto}:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/01_feature_selection_primera_comparacion} 

}

\caption{Comparacion bajo logística (I)}\label{fig:unnamed-chunk-37}
\end{figure}

Como primera impresión, y dada la escala del eje Y, la diferencia en
cuanto a tasa de fallos y AUC se refiere es muy pequeña, concretamente
de 0.01, aproximadamente (0.21 en tasa de fallos y 0.79 en AUC frente a
0.22 y 0.78).

Por tanto, dado que la diferencia entre ambos \emph{datasets} es
pequeña, \textbf{de cara al resto de la práctica se trabajó con el
subconjunto del 40 \%}. No obstante, en los últimos apartados (y una vez
tuneados los modelos), se realiza una última comparación con el fichero
original, comprobando de este modo si el orden de los algoritmos se
conserva. Además, de cara a la evaluación de los modelos se ha tomado la
decisión de utilizar el resto de observaciones del \emph{dataset}
original como conjunto \emph{test} \textbf{con el que evaluar (como
primera impresión) la precisión del modelo}.

\hypertarget{selecciuxf3n-de-los-mejores-sets-de-variables}{%
\subsubsection{4.6.2 Selección de los mejores sets de
variables}\label{selecciuxf3n-de-los-mejores-sets-de-variables}}

Una vez realizada la comparación con el \emph{dataset} original, nos
centramos en la selección de variables obtenida por el subconjunto,
tanto por \emph{stepwise AIC/BIC} como por RFE. En primer lugar, y
remontándonos al daigrama de cajas anterior, debemos recordar la
diferencia en el número de variables entre \emph{stepwise AIC} y
\emph{BIC}: con 14 y 9 variables, respectivamente, \textbf{la tasa de
fallos es prácticamente idéntica, y en relación al valor AUC, la
diferencia es de tan solo unas milésimas, pues ambos modelos se sitúan
en torno a 0.78}. Por tanto, el hecho de incluir demasiadas variables
\textbf{no afecta en gran medida al modelo, lo cual puede traducirse en
un sobreajuste en el resto de algoritmos}.

En consecuencia, en ambas selecciones \textbf{probamos a eliminar las
variables categóricas menos relevantes, concretamente aquellas con un
menor poder predictivo}, tal y como pudimos observar en el apartado de
depuración, gracias al \textbf{Valor de Información}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  En el caso de \emph{stepwise AIC}, eliminamos los campos
  \emph{baseline\_cvd}, \emph{asa\_status.0} y \emph{baseline\_diabetes}
  (iv: 0.04, 0.0062 y 0.0013, respectivamente).
\item
  En el caso de \emph{stepwise BIC}, eliminamos el campo
  \emph{asa\_status.0} (iv: 0.0062).
\end{enumerate}

\begin{verbatim}
##     n      setpwise_aic      stepwise_bic
## 1   1     mortality_rsi     mortality_rsi
## 2   2     ccsMort30Rate     ccsMort30Rate
## 3   3               bmi               bmi
## 4   4           month.8           month.8
## 5   5             dow.0             dow.0
## 6   6               Age               Age
## 7   7       moonphase.0       moonphase.0
## 8   8           month.0 baseline_osteoart
## 9   9 baseline_osteoart                 -
## 10 10 baseline_charlson                 -
## 11 11          ahrq_ccs                 -
\end{verbatim}

Analicemos tanto la tasa de fallos como el valor AUC:

\begin{figure}[h!]

{\centering \includegraphics[width=0.97\linewidth,height=0.97\textheight,]{./charts/01_feature_selection_segunda_comparacion} 

}

\caption{Comparacion bajo logística (II)}\label{fig:unnamed-chunk-39}
\end{figure}

Incluso reduciendo el número de variables en ambos casos, aunque la tasa
de fallos aumente o el valor auc disiminuya ligeramente, la diferencia
no es muy significativa.

No obstante, uno de los aspectos que más ha llamada la atención ha sido
la selección de variables RFE con Random Forest, donde con tan solo 5
variables la accuracy aumentaba hasta cerca del 90 \%, mucho más alto
que un modelo logístico. Por tanto, ¿Y si entrenamos un pequeño modelo
random forest para observar la importancia de las variables tanto en
\emph{stepwise AIC} como en \emph{BIC}?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} mtry (sqrt(numero de variables), por defecto)}
\NormalTok{rf\_modelo\_bic <{-}}\StringTok{ }\KeywordTok{train\_rf\_model}\NormalTok{(surgical\_dataset, formula.candidato.bic}\FloatTok{.2}\NormalTok{, }\DataTypeTok{ntree =} \DecValTok{1000}\NormalTok{, }
                                \DataTypeTok{grupos =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repe =} \DecValTok{5}\NormalTok{, }\DataTypeTok{nodesize =} \DecValTok{10}\NormalTok{, }\DataTypeTok{seed =} \DecValTok{1234}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/01_feature_selection_comparacion_random_forest} 

}

\caption{Importancia variables Random Forest (AIC y BIC)}\label{fig:unnamed-chunk-41}
\end{figure}

Analizando el gráfico de importancia, caben destacar cuatro principales
variables, \textbf{las cuales coinciden en ambos modelos, tanto en
\emph{stepwise AIC} como \emph{BIC}}: \emph{Age}, \emph{mortality\_rsi},
\emph{bmi} y \emph{ccsMort30Rate}. Por otro lado, en el modelo
\emph{AIC} cabe destacar, además, la variable \emph{ahrq\_ccs}. En
relación con el modelo \emph{BIC}, el contraste entre las cuatro
primeras variables y el resto de \emph{features} es más significativo,
de las cuales caben destacar \emph{baseline\_osteoart} y \emph{month.8},
con un nivel de importancia similar en el \emph{Random Forest}.

Por tanto, vista la importancia que presentan las variables en el modelo
\emph{Random Forest}, cabría preguntarse si realmente es necesario un
modelo con 11 u 8 variables, como es el caso de \emph{stepwise AIC} o
\emph{BIC}, respectivamente. Es decir, ¿Y si reducimos el modelo a las
4-5 variables más relevantes? Por ejemplo, una última comparación bajo
logística consistiría en (y en base al gráfico de importancia anterior)
analizar un modelo con las cuatro variables más importantes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Age}, \emph{mortality\_rsi}, \emph{bmi} y \emph{ccsMort30Rate}.
\end{enumerate}

Otro modelo logístico con las cinco variables más relevantes del
\emph{set} de variables \emph{BIC}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \emph{Age}, \emph{mortality\_rsi}, \emph{bmi}, \emph{ccsMort30Rate} y
  \emph{ahrq\_css} (coincide con con el modelo obtenido en \emph{RFE}
  con \emph{Random Forest}, por lo que lo denotamos como \emph{RFE RF
  TOP 5 (AIC TOP 5)}).
\end{enumerate}

Además de otros dos modelos con las cinco variables más relevantes en el
\emph{set} de variables \emph{AIC} (dado que \emph{baseline\_osteoart} y
\emph{month.8} son muy similares en cuanto a importancia se refiere,
realizamos la prueba con ambos \emph{sets}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \emph{Age}, \emph{mortality\_rsi}, \emph{bmi}, \emph{ccsMort30Rate} y
  \emph{baseline\_osteoart}.
\item
  \emph{Age}, \emph{mortality\_rsi}, \emph{bmi}, \emph{ccsMort30Rate} y
  \emph{month.8}.
\end{enumerate}

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/01_feature_selection_comparacion_final} 

}

\caption{Comparacion bajo logística (III)}\label{fig:unnamed-chunk-42}
\end{figure}

Como podemos comprobar, aunque la tasa de fallos sea similar tanto con
\emph{basline\_osteoart} como con \emph{month.8}, el valor AUC es
ligeramente superior, incluso cercano a los valores obtenidos por la
selección \emph{AIC}, \emph{BIC} original. Por otro lado, con tan solo
cuatro variables, aunque el valor AUC sea ligeramente inferior con
respecto al resto de \emph{sets}, la diferencia no es tan significativa
(0.755 a 0.775, aproximadamente).

Por tanto, de cara al resto de modelos \textbf{emplamos dos modelos
candidatos}:

\begin{verbatim}
##   n       modelo1       modelo2
## 1 1           Age           Age
## 2 2 mortality_rsi mortality_rsi
## 3 3           bmi           bmi
## 4 4 ccsMort30Rate ccsMort30Rate
## 5 5       month.8             -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-}{-} Variables de los modelos candidatos}
\CommentTok{\#{-}{-}  Modelo 1}
\NormalTok{var\_modelo1 <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"mortality\_rsi"}\NormalTok{, }\StringTok{"ccsMort30Rate"}\NormalTok{, }\StringTok{"bmi"}\NormalTok{, }\StringTok{"month.8"}\NormalTok{, }\StringTok{"Age"}\NormalTok{)}

\CommentTok{\#{-}{-} Modelo 2}
\NormalTok{var\_modelo2 <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"mortality\_rsi"}\NormalTok{, }\StringTok{"bmi"}\NormalTok{, }\StringTok{"month.8"}\NormalTok{, }\StringTok{"Age"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tuneo-y-comparaciuxf3n-final}{%
\subsection{4.7 Tuneo y comparación
final}\label{tuneo-y-comparaciuxf3n-final}}

Finalmente, realizamos una última comparación de ambos modelos
candidatos, \textbf{aumentando la validación cruzada de 5 a 10
repeticiones}:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/01_feature_selection_comparacion_5_10_rep} 

}

\caption{Comparacion final logística (5-10 rep.)}\label{fig:unnamed-chunk-45}
\end{figure}

A primera vista, al aumentar el número de repeticiones, la varianza en
ambos modelos es muy similar, con una ventaja en el segundo modelo en
cuanto a AUC se refiere (0.775 frente a 0.755, aproximadamente). Por
otro lado, si nos fijamos en las estadísticas obtenidas a partir de los
datos \emph{test}:

\begin{verbatim}
##                 modelo precision sensibilidad especificidad valor_pred_pos
## 1 Modelo 1 (BIC TOP 5)    0.7940      0.67730       0.81011        0.33013
## 2     Modelo 2 (top 4)    0.7824      0.66707       0.79434        0.25194
##   valor_pred_neg
## 1        0.94783
## 2        0.95830
\end{verbatim}

Observamos que la sensibilidad de ambos modelos ronda el 66-67 \%,
mientras que la especificidad aumenta hasta el 80 \%, aproximadamente.
Es decir, con un modelo lineal como es el caso de la regresión logística
conseguimos clasificar la mayoría de los pacientes a 0 (sin
complicaciones). Sin embargo, tanto el valor predictivo positivo como la
sensibilidad nos indican justo con lo contrario en relación a los
pacientes con complicaciones:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Valor predictivo positivo} (en torno al 25-33 \% en ambos
  modelos), lo que se traduce en un alto número de falsos positivos.
\item
  \textbf{Sensbilidad} (en torno al 66-67 \% en ambos modelos), lo que
  se traduce en un alto número de falsos negativos.
\end{enumerate}

\hypertarget{modelos-iniciales-con-h2o}{%
\section{5. Modelos iniciales con H2O}\label{modelos-iniciales-con-h2o}}

Una realizada la selección de variables y decantarnos por dos posibles
\emph{sets} de variables, antes de comenzar con el tuneo de
hiperparámetros, \textbf{realizamos un modelo \emph{autoML} por cada
\emph{set} de variables}. De este modo, podremos comprobar, de forma
orientativa:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Qué modelo o modelos son los más adecuados, tanto a nivel AUC como en
  tasa de fallos.
\item
  Del mejor o mejores modelos, qué parámetros se ha empleado.
\end{enumerate}

\hypertarget{modelo-1-bic-top-5}{%
\subsection{5.1 Modelo 1 (BIC TOP 5)}\label{modelo-1-bic-top-5}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#   Modelo 1}
\NormalTok{aml\_}\DecValTok{1}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{h2o.automl}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ var\_modelo1, }\DataTypeTok{y =}\NormalTok{ target, }
                    \DataTypeTok{training\_frame =}\NormalTok{ surgical\_dataset\_h, }\DataTypeTok{nfolds =} \DecValTok{5}\NormalTok{, }\DataTypeTok{seed =} \DecValTok{1234}\NormalTok{,}
                    \DataTypeTok{balance\_classes =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{keep\_cross\_validation\_predictions =} \OtherTok{TRUE}\NormalTok{,}
                    \DataTypeTok{max\_models =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                              model_id       auc   logloss mean_per_class_error
1                         GBM_5_AutoML_20210422_174850 0.9211350 0.2591059            0.1608837
2  StackedEnsemble_BestOfFamily_AutoML_20210422_174850 0.9208431 0.2647496            0.1603029
3                         GBM_1_AutoML_20210422_174850 0.9200581 0.2690935            0.1659645
4     StackedEnsemble_AllModels_AutoML_20210422_174850 0.9200033 0.2647962            0.1676764
5                         GBM_2_AutoML_20210422_174850 0.9190857 0.2695043            0.1648937
6                     XGBoost_3_AutoML_20210422_174850 0.9189663 0.2613378            0.1651901
7       XGBoost_grid__1_AutoML_20210422_174850_model_1 0.9189402 0.2629065            0.1675371
8           GBM_grid__1_AutoML_20210422_174850_model_1 0.9187735 0.2763313            0.1689585
9                         GBM_3_AutoML_20210422_174850 0.9175951 0.2725404            0.1757392
10                    XGBoost_2_AutoML_20210422_174850 0.9169047 0.2717032            0.1707306
\end{verbatim}

En un primer análisis, de todos los modelos \emph{autoML} creados,
\textbf{los modelos \emph{gradient boosting}, \emph{ensemble} y
\emph{xgboost} obtienen los mejores resultados}, con un AUC en torno a
0.92, así como un error medio por cada clase de 0.16-0.17,
aproximadamente. Por otro lado, si analizamos los parámetros del mejor
modelo, concretamente \emph{gradient boosting}:

\begin{verbatim}
##                 modelo ntrees max_depth sample_rate col_sample_rate
## 1 Modelo 1 (BIC TOP 5)     82        15         0.8             0.8
##   col_sample_rate_per_tree
## 1                      0.8
\end{verbatim}

Analizando los parámetros, por lo general \emph{h2o} \textbf{opta por un
modelo \emph{gbm} sencillo, con un número bajo de árboles (inferior a
100), un profundidad moderada en cada árbol (15), además de sortear no
solo observaciones, sino además variables (en torno al 80 \%)}.

\hypertarget{modelo-2-aic-bic-top-4}{%
\subsection{5.2 Modelo 2 (AIC-BIC top 4)}\label{modelo-2-aic-bic-top-4}}

\begin{verbatim}
                                              model_id       auc   logloss  mean_per_class_error
1                         GBM_5_AutoML_20210422_182719 0.9096419 0.2700148            0.1646700
2  StackedEnsemble_BestOfFamily_AutoML_20210422_182719 0.9088350 0.2712222            0.1666722
3     StackedEnsemble_AllModels_AutoML_20210422_182719 0.9083655 0.2708531            0.1683963
4                     XGBoost_3_AutoML_20210422_182719 0.9083145 0.2683037            0.1735557
5                         GBM_1_AutoML_20210422_182719 0.9075638 0.2769396            0.1730899
6                         GBM_2_AutoML_20210422_182719 0.9072236 0.2767946            0.1770094
7       XGBoost_grid__1_AutoML_20210422_182719_model_1 0.9065834 0.2696259            0.1678335
8                     XGBoost_2_AutoML_20210422_182719 0.9064648 0.2756899            0.1671682
9                     XGBoost_1_AutoML_20210422_182719 0.9047755 0.2746863            0.1725274
10          GBM_grid__1_AutoML_20210422_182719_model_1 0.9044822 0.3115999            0.1720491
\end{verbatim}

Nuevamente, con el segundo \emph{set} de variables continua optando por
un modelo \emph{grandient boosting}, con un AUC muy similar al primer
\emph{set} de variables (0.90 frente a 0.92). Además, si analizamos los
parámetros del mejor modelo:

\begin{verbatim}
##                    modelo ntrees max_depth sample_rate col_sample_rate
## 1    Modelo 1 (BIC TOP 5)     82        15         0.8             0.8
## 2 Modelo 2 (AIC-BIC-top4)     72        15         0.8             0.8
##   col_sample_rate_per_tree
## 1                      0.8
## 2                      0.8
\end{verbatim}

Salvo el número de árboles, el resto de parámetros coinciden. Por tanto,
de cara a la elaboración de los modelos, \textbf{comprobaremos si el
mejor modelo en ambos \emph{sets} es, efectivamente, un modelo
\emph{gbm}, o si al menos está entre los mejores}.

\hypertarget{redes-neuronales}{%
\section{6. Redes neuronales}\label{redes-neuronales}}

\hypertarget{modelo-1-bic-top-5-1}{%
\subsection{6.1 Modelo 1 (BIC TOP 5)}\label{modelo-1-bic-top-5-1}}

A continuación, procedemos con el tuneado del primer modelo
\emph{Machine Learning}: la red neuronal, comenzando con el primer
\emph{set} (5 variables input). En primera instancia, analicemos el
número de nodos necesario para obtener entre 20 y 30 observaciones por
parámetro, teniendo 5 variables \emph{input}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Con 20 observaciones por parámetro: \[
  h * (k + 1) + h + 1 = 5854 / 20 = 292 \text{ parámetros}
  \] \[
  \text{Con } k = 5 \text{ variables input, entonces: } 7 * h + 1 = 292. \text{ Es decir}, 41-42 \text{ nodos}
  \]
\item
  Con 30 observaciones por parámetro: \[
  h * (k + 1) + h + 1 = 5854 / 30 = 195 \text{ parámetros}
  \] \[
  \text{Con } k = 5 \text{ variables input, entonces: } 7 * h + 1 = 195 \text{. Es decir}, 27-28 \text{ nodos}
  \]
\end{enumerate}

En primera instancia, dado que dispone de tan solo cinco variables ,
probablemente no serán necesarios tantos nodos. No obstante, realizamos
un primer tuneo con varios tamaños (desde 5 hasta 40):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{size.candidato}\FloatTok{.1}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{40}\NormalTok{)}
\NormalTok{decay.candidato}\FloatTok{.1}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.001}\NormalTok{)}

\NormalTok{cvnnet.candidato}\FloatTok{.1}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{cruzadaavnnetbin}\NormalTok{(}\DataTypeTok{data=}\NormalTok{surgical\_dataset,}\DataTypeTok{vardep=}\NormalTok{target,}
                                       \DataTypeTok{listconti=}\NormalTok{var\_modelo1, }\DataTypeTok{listclass=}\KeywordTok{c}\NormalTok{(}\StringTok{""}\NormalTok{),}
                                       \DataTypeTok{grupos=}\DecValTok{5}\NormalTok{,}\DataTypeTok{sinicio=}\DecValTok{1234}\NormalTok{,}\DataTypeTok{repe=}\DecValTok{5}\NormalTok{, }\DataTypeTok{size=}\NormalTok{size.candidato}\FloatTok{.1}\NormalTok{,}
                                       \DataTypeTok{decay=}\NormalTok{decay.candidato}\FloatTok{.1}\NormalTok{,}\DataTypeTok{repeticiones=}\DecValTok{5}\NormalTok{,}\DataTypeTok{itera=}\DecValTok{200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    id size decay  Accuracy
## 1   1   25 0.010 0.8950798
## 2   2   30 0.010 0.8947370
## 3   3   20 0.010 0.8945332
## 4   4   35 0.010 0.8939514
## 5   5   30 0.001 0.8934057
## 6   6   40 0.010 0.8921747
## 7   7   25 0.001 0.8919025
## 8   8   35 0.001 0.8914928
## 9   9   15 0.010 0.8905699
## 10 10   20 0.001 0.8899210
## 11 11   40 0.001 0.8892375
## 12 12   25 0.100 0.8862640
## 13 13   35 0.100 0.8855127
## 14 14   30 0.100 0.8847618
## 15 15   20 0.100 0.8841129
## 16 16   40 0.100 0.8837363
## 17 17   15 0.100 0.8829166
## 18 18   15 0.001 0.8794649
## 19 19   10 0.010 0.8793648
## 20 20   10 0.100 0.8653218
## 21 21   10 0.001 0.8557885
## 22 22    5 0.010 0.8237056
## 23 23    5 0.100 0.8129454
## 24 24    5 0.001 0.8113744
\end{verbatim}

Analizando la tabla anterior, empleando un \emph{decay} o
\emph{learning\_rate} de 0.01, \textbf{el modelo obtiene muy buenos
resultados, mejores incluso que la regresión logística, un indicativo de
la no linealidad de las variables}. Por otro lado, y en relación con el
número de nodos ¿Merece aumentar hasta 20, 30 o incluso 40 nodos? En
vista a los resultados, no lo parece. A modo de ejemplo, con tan solo 15
nodos y un \emph{decay} de 0.01, el valor de \emph{accuracy} alcanza un
89 \%, mientras que con 20, 30 o 40 nodos, la diferencia es de tan solo
unas milésimas (no estamos ganando lo suficiente como para decantarnos
por un modelo más complejo). Por tanto, una buena alternativa sería
emplear 15 nodos (10 tampoco sería una mala opción, aunque comienza a
decaer hasta el 87 \%).

No obstante, con un \emph{decay} de 0.01, analicemos tanto el sesgo como
la varianza en base al número de nodos:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/02_comparacion_tasa_auc_modelo1} 

}

\caption{Comparacion avnnet modelo 1}\label{fig:unnamed-chunk-52}
\end{figure}

Como primera impresión, y en base al mejor AUC, un modelo con 20, 25 o
30 nodos sería una buena alternativa, de no ser por un aspecto clave:
\textbf{la escala de los ejes}. En relación con la tasa de fallos, la
diferencia de error entre un modelo con 15 nodos y un modelo con 20 es
muy pequeña (de 0.11 a 0.105), además de que la varianza del modelo con
15 nodos (elevada desde un punto de vista gráfico), la escala del eje
puede llevar a engaño, dado que sus valores de error oscilan entre
0.1075 y 0.11. Por otro lado, la diferencia del AUC entre un modelo con
15 nodos y un modelo con 20 es muy pequeña (en torno 0.8925-0.895 en el
caso de 15 nodos y 0.895-0.90 en el caso de 20 nodos).

De hecho, si aumentamos el número de repeticiones a 10:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/02_comparacion_tasa_auc_modelo1_10rep} 

}

\caption{Comparacion avnnet modelo 1 (10 rep.)}\label{fig:unnamed-chunk-53}
\end{figure}

Comprobamos que tanto el orden de los diferentes modelos como su
varianza se mantienen prácticamente constante. En conclusión, con el
primer \emph{set} de variables \textbf{nos decantamos por un modelo de
red con 15 nodos}, dado que la ganancia que supone al aumentar el número
de nodos, tanto en AUC como en tasa de fallos, es muy pequeña.

A continuación, y aumentando a 10 el número de repeticiones, tuneamos el
número de iteraciones:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/02_comparacion_tasa_auc_modelo1_10rep_iters} 

}

\caption{Comparacion avnnet modelo 1 (iteraciones)}\label{fig:unnamed-chunk-54}
\end{figure}

Analizando los resultados, \textbf{bien es cierto que conforme aumenta
el número de iteraciones, tanto la tasa de fallos como el AUC comienzan
a estabilizarse}. No obstante, la ganancia que supone aumentar el número
de iteraciones es muy pequeña. A modo de ejemplo, de 200 a 300
iteraciones, la tasa de fallos mejora de 0.11 a 0.105, aproximadamente,
mientras que el valor AUC apenas se ve afectado. Por tanto,
\textbf{mantenemos el número de iteraciones a 200}.

\hypertarget{modelo-2-aic-bic-top-4-1}{%
\subsection{6.2 Modelo 2 (AIC-BIC TOP
4)}\label{modelo-2-aic-bic-top-4-1}}

A continuación, realizamos los mismos pasos con el segundo \emph{set} de
variables candidato. En primer lugar, y dado que disponemos de 4
variables \emph{input}, para obtener 20 o 30 observaciones por
parámetros necesitamos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Con 20 observaciones por parámetro:
\end{enumerate}

\[
\text{Con } k = 4 \text{ variables input, entonces: } 6 * h + 1 = 292. \text{ Es decir}, 48-49 \text{ nodos}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Con 30 observaciones por parámetro: \[
  \text{Con } k = 4 \text{ variables input, entonces: } 6 * h + 1 = 195 \text{. Es decir}, 27-28 \text{ nodos}
  \]
\end{enumerate}

Con un primer cálculo, necesitaríamos un elevado número de nodos para
obtener, al menos, 20 o 30 observaciones por parámetro. Sin embargo, si
nos fijamos en lo empírico:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{size.candidato}\FloatTok{.2}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{45}\NormalTok{)}
\NormalTok{decay.candidato}\FloatTok{.2}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.001}\NormalTok{)}

\NormalTok{cvnnet.candidato}\FloatTok{.1}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{cruzadaavnnetbin}\NormalTok{(}\DataTypeTok{data=}\NormalTok{surgical\_dataset,}\DataTypeTok{vardep=}\NormalTok{target,}
                                       \DataTypeTok{listconti=}\NormalTok{var\_modelo2, }\DataTypeTok{listclass=}\KeywordTok{c}\NormalTok{(}\StringTok{""}\NormalTok{),}
                                       \DataTypeTok{grupos=}\DecValTok{5}\NormalTok{,}\DataTypeTok{sinicio=}\DecValTok{1234}\NormalTok{,}\DataTypeTok{repe=}\DecValTok{5}\NormalTok{, }\DataTypeTok{size=}\NormalTok{size.candidato}\FloatTok{.1}\NormalTok{,}
                                       \DataTypeTok{decay=}\NormalTok{decay.candidato}\FloatTok{.1}\NormalTok{,}\DataTypeTok{repeticiones=}\DecValTok{5}\NormalTok{,}\DataTypeTok{itera=}\DecValTok{200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    id size decay  Accuracy
## 1   1   20 0.001 0.9026305
## 2   2   35 0.001 0.9021868
## 3   3   25 0.001 0.9020152
## 4   4   30 0.001 0.9017081
## 5   5   25 0.010 0.9007853
## 6   6   30 0.010 0.8998632
## 7   7   15 0.001 0.8997609
## 8   8   45 0.001 0.8995550
## 9   9   20 0.010 0.8994869
## 10 10   45 0.010 0.8994521
## 11 11   40 0.001 0.8994190
## 12 12   15 0.010 0.8992479
## 13 13   35 0.010 0.8989061
## 14 14   40 0.010 0.8986332
## 15 15   10 0.010 0.8946358
## 16 16   10 0.001 0.8906726
## 17 17   25 0.100 0.8897164
## 18 18   40 0.100 0.8896133
## 19 19   20 0.100 0.8895444
## 20 20   35 0.100 0.8894082
## 21 21   30 0.100 0.8888957
## 22 22   45 0.100 0.8888273
## 23 23   15 0.100 0.8884856
## 24 24   10 0.100 0.8855818
## 25 25    5 0.010 0.8447890
## 26 26    5 0.001 0.8358688
## 27 27    5 0.100 0.8303017
\end{verbatim}

En un primer resultado, \textbf{con un parámetro de regularización
pequeño obtenemos buenos resultados}. A modo de ejemplo, llama la
atención modelos de red con 20, 25 o 30 nodos que alcanzan un
\emph{Accuracy} de 0.90. Sin embargo, conforme descendemos en la tabla
(y con ello, el número de nodos) comprobamos que la diferencia no es muy
significativa: con tan solo 15 o 10 nodos y un \emph{decay} de 0.01, el
valor de \emph{Accuracy} tan solo empeora en 0.89. Es decir, a simple
vista el hecho de aumentar el número de nodos \textbf{no compensa la
ganancia de \emph{Accuracy}}.

No obstante, analicemos tanto el sesgo como la varianza con los mejores
modelos:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Probamos con 15{-}20{-}25{-}30 nodos y decay 0.001; 10{-}15 nodos y decay 0.01}
\NormalTok{size.candidato}\FloatTok{.2}\NormalTok{  <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{)}
\NormalTok{decay.candidato}\FloatTok{.2}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.001}\NormalTok{, }\FloatTok{0.001}\NormalTok{, }\FloatTok{0.001}\NormalTok{, }\FloatTok{0.001}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/02_comparacion_tasa_auc_modelo2_5rep} 

}

\caption{Comparacion avnnet modelo 2}\label{fig:unnamed-chunk-58}
\end{figure}

En un primer resultado, \textbf{la ganancia de error que supone reducir
el número de nodos a 10 (y con un \emph{decay} de 0.01), es muy pequeña:
de 0.09 con 25 nodos a 0.105 con tan solo 10 nodos}. Del mismo modo
sucede con el área bajo la curva, donde la mejora con 15 o 30 nodos es
de tan solo unas milésimas de diferencia, lo cual puede depender de
condiciones azarosas como la semilla de aleatoriedad. No obstante, con
10, 15 y 20 nodos, echemos un vistazo al modelo con 10 repeticiones:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/02_comparacion_tasa_auc_modelo2_5rep} 

}

\caption{Comparacion avnnet modelo 2 (10 rep.)}\label{fig:unnamed-chunk-59}
\end{figure}

Incluso aumentando a 10 repeticiones, el orden de los modelos no cambia.
Por tanto, dada su simplicidad y la poca ganancia de error que supone,
\textbf{elegimos el modelo con 10 nodos y \emph{decay} 0.01}.

Por último, tuneamos el número de iteraciones:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/02_comparacion_tasa_auc_modelo2_10rep_iters} 

}

\caption{Comparacion avnnet modelo 2 (iteraciones)}\label{fig:unnamed-chunk-60}
\end{figure}

Del mismo modo que sucedía con el primer \emph{set} de variables,
\textbf{aumentar el número de iteraciones no supone una mejoría
significativa}, por lo que lo mantenemos a 200.

\hypertarget{comparaciuxf3n-final}{%
\subsection{6.3 Comparación final}\label{comparaciuxf3n-final}}

Una vez obtenidos ambos modelos, con los datos \emph{test} realizamos
las primeras predicciones:

\begin{verbatim}
##                 modelo precision sensibilidad especificidad valor_pred_pos
## 1 Modelo 1 (BIC TOP 5)    0.8992       0.9471        0.8897         0.6305
## 2     Modelo 2 (top 4)    0.9032       0.9639        0.8913         0.6351
##   valor_pred_neg
## 1         0.9883
## 2         0.9921
\end{verbatim}

En primera instancia, \textbf{con un modelo de red sencillo conseguimos
mejorar prácticamente en todos los aspectos}. No obstante, cabe destacar
el valor predictivo positivo, que pese a su mejoría (del 20-30 al 63
\%), \textbf{continua existiendo un alto porcentaje de falsos
positivos}.

En conclusión, a la vista de los resultados obtenidos, tanto en tasa de
fallos como en AUC, \textbf{los modelos de red mejoran
significativamente los resultados del modelo}, un claro indicio de la
\textbf{no linealidad con la variable objetivo}. En relación con ambos
\emph{set} de variables, el hecho de añadir una variable \emph{input}
adicional en el primer modelo \textbf{no hace mejorar significativamente
sus resultados}:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/comparativas/02_log_avnnet_tasa} 

}

\caption{Comparacion tasa fallos log-avnnet}\label{fig:unnamed-chunk-62}
\end{figure}
\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/comparativas/02_log_avnnet_auc} 

}

\caption{Comparacion AUC log-avnnet}\label{fig:unnamed-chunk-63}
\end{figure}

\hypertarget{bagging}{%
\section{7. Bagging}\label{bagging}}

\hypertarget{selecciuxf3n-del-nuxfamero-de-uxe1rboles}{%
\subsection{7.1 Selección del número de
árboles}\label{selecciuxf3n-del-nuxfamero-de-uxe1rboles}}

Continuando con el modelo \emph{bagging}, de forma previa al tuneo de
hipérparametros, debemos fijar un número de árboles (ntrees) para ambos
\emph{sets} de variables, un valor mínimo a partir del cual el error OOB
( \emph{Out of bag error} ) se estabiliza:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} Seleccion del numero de arboles}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\CommentTok{\#{-}{-} Modelo 1}
\NormalTok{rfbis}\FloatTok{.1}\NormalTok{<{-}}\KeywordTok{randomForest}\NormalTok{(}\KeywordTok{factor}\NormalTok{(target)}\OperatorTok{\textasciitilde{}}\NormalTok{mortality\_rsi}\OperatorTok{+}\NormalTok{ccsMort30Rate}\OperatorTok{+}\NormalTok{bmi}\OperatorTok{+}\NormalTok{month}\FloatTok{.8}\OperatorTok{+}\NormalTok{Age,}
                      \DataTypeTok{data=}\NormalTok{surgical\_dataset,}
                      \DataTypeTok{mtry=}\NormalTok{mtry}\FloatTok{.1}\NormalTok{,}\DataTypeTok{ntree=}\DecValTok{5000}\NormalTok{,}\DataTypeTok{nodesize=}\DecValTok{10}\NormalTok{,}\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}

\CommentTok{\#{-}{-} Modelo 2}
\NormalTok{rfbis}\FloatTok{.2}\NormalTok{<{-}}\KeywordTok{randomForest}\NormalTok{(}\KeywordTok{factor}\NormalTok{(target)}\OperatorTok{\textasciitilde{}}\NormalTok{Age}\OperatorTok{+}\NormalTok{mortality\_rsi}\OperatorTok{+}\NormalTok{bmi}\OperatorTok{+}\NormalTok{month}\FloatTok{.8}\NormalTok{,}
                      \DataTypeTok{data=}\NormalTok{surgical\_dataset,}
                      \DataTypeTok{mtry=}\NormalTok{mtry}\FloatTok{.2}\NormalTok{,}\DataTypeTok{ntree=}\DecValTok{5000}\NormalTok{,}\DataTypeTok{nodesize=}\DecValTok{10}\NormalTok{,}\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/OOB_1} 

}

\caption{OOB Error (Modelos 1 y 2) (I)}\label{fig:unnamed-chunk-65}
\end{figure}

Analizando el error \emph{Out of bag}, \textbf{en ambos modelos el error
se estabiliza con aproximadamente 1000 árboles, obteniendo valores de
error inferiores a 0.1, similares a los obtenidos en los modelos de
red}. Si hacemos \emph{zoom} sobre el gráfico:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/OOB_2} 

}

\caption{OOB Error (Modelos 1 y 2) (II)}\label{fig:unnamed-chunk-66}
\end{figure}

Observamos como a partir de 200-250 árboles, el error comienza a
situarse por debajo de 0.10. Sin embargo, \textbf{no es hasta los
900-1000 árboles cuando el error prácticamente se estabiliza, a partir
del cual se detectan ciertas fluctuaciones (subidas o bajadas en el
error), aunque de forma aleatoria}. Por tanto, \textbf{para ambos
modelos escogemos 900 como número de árboles}.

\hypertarget{modelo-1}{%
\subsection{7.2 Modelo 1}\label{modelo-1}}

En un primer comienzo, hagamos un repaso previo de los parámetros a
tunear en un modelo \emph{bagging}, junto con el número de árboles:

\emph{mtry}: número de variables sorteadas aleatoriamente en cada
división del árbol. Dado que se trata de un modelo \emph{bagging},
\textbf{establecemos en dicho parámetro el número total de variables
independientes del modelo}.

\emph{nodesize}: tamaño máximo de nodos finales.

\emph{sampsize}: número de observaciones seleccionadas aleatoriamente
(con o sin reemplazamiento) para la construcción del árbol.

\emph{replace}: si el sorteo anterior se realiza con o sin
reemplazamiento (por defecto, con reemplazamiento).

Dado que \emph{mtry} debe corresponder con el número de variables
\emph{input} y el número de árboles ya está definido, quedan por tunear
tanto \emph{nodesize} como \emph{sampsize} y \emph{replace}. Por tanto,
\textbf{comenzamos ajustando tanto el tamaño de la submuestra como el
tamaño máximo de nodos finales} (en el caso del parámetro
\emph{replace}, una vez obtenidos los modelos \emph{bagging} para ambos
\emph{sets}, con el mejor de ambos probaremos a seleccionar muestras sin
reemplazamiento).

Antes de tunear \emph{sampsize}, y dado que estamos trabajando con
validación cruzada, \textbf{dispondremos de menos observaciones para
construir el modelo, de forma que debemos establecer un tamaño máximo
sobre dicho parámetro}. Concretamente, dado que disponemos de 5854
observaciones y 5 grupos de validación cruzada, cada grupo tendrá
disponible 5854 * (4/5) \textasciitilde{} 4683 observaciones, siendo el
tamaño máximo de muestra que podemos probar:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-} Redondeamos 4683 a 4600}
\NormalTok{sampsizes}\FloatTok{.1}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{2000}\NormalTok{, }\DecValTok{3000}\NormalTok{, }\DecValTok{4600}\NormalTok{)}
\NormalTok{nodesizes}\FloatTok{.1}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{)}

\NormalTok{bagging\_modelo1 <{-}}\StringTok{ }\KeywordTok{tuneo\_bagging}\NormalTok{(surgical\_dataset, }\DataTypeTok{target =}\NormalTok{ target,}
                                 \DataTypeTok{lista.continua =}\NormalTok{ var\_modelo1,}
                                 \DataTypeTok{nodesizes =}\NormalTok{ nodesizes}\FloatTok{.1}\NormalTok{,}
                                 \DataTypeTok{sampsizes =}\NormalTok{ sampsizes}\FloatTok{.1}\NormalTok{, }\DataTypeTok{mtry =}\NormalTok{ mtry}\FloatTok{.1}\NormalTok{,}
                                 \DataTypeTok{ntree =}\NormalTok{ n.trees}\FloatTok{.1}\NormalTok{, }\DataTypeTok{grupos =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repe =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación, por cada combinación \emph{nodesize} - \emph{sampsize}
mostramos el promedio tanto de la tasa de fallos como de AUC:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/03_distribucion_auc_tasa_fallos_modelo1} 

}

\caption{Distribucion de la tasa de fallos y AUC por sampsize y nodesize (Modelo 1) (I)}\label{fig:unnamed-chunk-68}
\end{figure}

En un primer análisis, caben destacar fundamentalmente dos aspectos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  En relación al \emph{nodesize}, tanto en tasa de fallos como en AUC,
  se obtienen buenos resultados en torno a un valor de 5-30,
  aproximadamente, pues a partir de 50-100 (árboles menos complejos), la
  precisión del modelo comienza a disminuir. Sin embargo, ¿Merece la
  pena aumentar la complejidad del árbol, disminuyendo el parámetro
  \emph{nodesize}? A simple vista, la diferencia entre un modelo de
  mayor complejidad ( \emph{nodesize} = 5 ), y un modelo de menor
  complejidad ( \emph{nodesize} = 20 ), es muy pequeña (punto rojo y
  verde). \textbf{Por tanto, una posibilidad sería decantarse por un
  \emph{nodesize} elevado, en torno a 20.}
\item
  Por otro lado, llama la atención el tuneo sobre el parámetro
  \emph{sampsize}. Bien es cierto que conforme aumenta el tamaño de la
  submuestra, mayor es la precisión. No obstante, ¿Qué diferencia existe
  entre un modelo en el que se utilizan todas las observaciones (
  \emph{sampsize} = 1 ) y un modelo con tan solo 500 o 1000 submuestras?
  No hay demasiada diferencia, en especial con un \emph{nodesize} en
  torno a 20, tal y como comentamos en el apartado anterior. De este
  modo, y gracias además al elevado número de árboles del que disponemos
  (900), \textbf{un valor \emph{sampsize} bajo permite no solo reducir
  el tiempo de cómputo para entrenar el modelo, sino además una mayor
  variedad en la construcción de cada uno de los árboles}.
\end{enumerate}

Es decir, para obtener un buen modelo \emph{bagging} \textbf{no es
necesario emplear todas las observaciones, ni tampoco árboles con
demasiada complejidad}, sino que con pocas muestras (en torno a 500), y
un \emph{nodesize} alto (en torno a 20), se obtienen muy buenos
resultados. Por tanto, una vez realizado el primer análisis, realizamos
una comparación (tanto del sesgo como de la varianza) sobre modelos
\emph{bagging} con un valor \emph{nodesize} pequeño (5), lo que se
traduce en árboles más complejos, frente a modelos con un valor alto (en
torno a 20). Del mismo modo, analizamos el sesgo y varianza con
diferentes valores de \emph{sampsize}, aumentando a 10 el número de
repeticiones para observar mejor su efecto:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nodesizes}\FloatTok{.1}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\CommentTok{\#{-}{-} Probamos con un sampsize entre 500 y 2000 observaciones (1 = todas las observaciones)}
\NormalTok{sampsizes}\FloatTok{.1}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1500}\NormalTok{, }\DecValTok{2000}\NormalTok{)}
\NormalTok{bagging\_modelo1\_}\DecValTok{5}\NormalTok{ <{-}}\StringTok{ }\KeywordTok{tuneo\_bagging}\NormalTok{(surgical\_dataset, }\DataTypeTok{target =}\NormalTok{ target,}
                                   \DataTypeTok{lista.continua =}\NormalTok{ var\_modelo1,}
                                   \DataTypeTok{nodesizes =}\NormalTok{ nodesizes}\FloatTok{.1}\NormalTok{,}
                                   \DataTypeTok{sampsizes =}\NormalTok{ sampsizes}\FloatTok{.1}\NormalTok{, }\DataTypeTok{mtry =}\NormalTok{ mtry}\FloatTok{.1}\NormalTok{,}
                                   \DataTypeTok{ntree =}\NormalTok{ n.trees}\FloatTok{.1}\NormalTok{, }\DataTypeTok{grupos =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repe =} \DecValTok{10}\NormalTok{,}
                                   \DataTypeTok{show\_nrnodes =} \StringTok{"yes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/03_distribucion_auc_tasa_fallos_modelo1_comp} 

}

\caption{Distribucion de la tasa de fallos y AUC por sampsize y nodesize (Modelo 1) (II)}\label{fig:unnamed-chunk-70}
\end{figure}

Nuevamente, sin fijarnos en la escala del eje, una posible opción a
decantarse sería elegir un modelo con \emph{nodesize} 5 y
\emph{sampsize} 1000, pues presenta el mayor valor AUC y una tasa de
fallos baja. No obstante, los valores del eje pueden llevarnos a engaño.
Por ejemplo, la diferencia entre este modelo y uno mucho más sencillo
como es el caso de \emph{nodesize} 20 y \emph{sampsize} en torno a
500-1000 (20 + 500 o 20 + 1000) es muy pequeña (hablamos de una tasa de
fallos en torno a 0.10 y un AUC de 0.91, por lo que la diferencia es del
órden de milésimas), incluso con un menor número de muestras la varianza
de los modelos se ve reducida en comparación con usar todas las
observaciones, aunque la diferencia (por las escalas del eje) sea menor.

Por tanto, dada la poca ganancia que presentan los modelos complejos,
\textbf{nos decantamos por un modelo \emph{bagging} con \emph{nodesize}
= 20 y un tamaño de muestra en torno a 500 y 1000}. Sobre ambos
candidatos, para controlar mejor el tamaño óptimo de la muestra,
probamos con una validación cruzada de 10 grupos y 20 repeticiones:

\begin{figure}[h!]

{\centering \includegraphics[width=0.99\linewidth,height=0.99\textheight,]{./charts/bagging/bis_03_comparacion_final_modelo1_5_10_folds} 

}

\caption{Distribucion de la tasa de fallos y AUC (Modelo 1) aumentando grupos y repeticiones}\label{fig:unnamed-chunk-71}
\end{figure}

Por lo general, incluso aumentando el número de grupos y repeticiones,
el orden de los modelos se mantiene idéntico, tanto en tasa de fallos
como en AUC. Por tanto, de los dos posibles modelos candidatos (con
\emph{sampsize} 500 o 1000), aunque la diferencia entre ambos, así como
el sesgo y varianza no sean muy significativas (dada la escala de los
ejes), \textbf{nos decantamos por un modelo con un tamaño de 1000
submuestras}.

\hypertarget{modelo-2}{%
\subsection{7.3 Modelo 2}\label{modelo-2}}

\end{document}
