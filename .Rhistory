t<-as.data.frame(tasa)
t$modelo<-prediccion
auc<-suppressMessages(auc(archi$obs,archi$proba))
t$auc<-auc
medias1<-rbind(medias1,t)
}
}
medias1$tipo <- c(rep("Logistica", 10),
rep("Original",  80),
rep("Logistica", 150),
rep("Ensamblado", 200))
medias1$modelo <- with(medias1,
reorder(modelo,tasa, mean))
ggplot(medias1, aes(x = modelo, y = tasa, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
medias1$modelo <- with(medias1,
reorder(modelo,auc, mean))
ggplot(medias1, aes(x = modelo, y = auc, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
#-- Pese a añadir mas de 3 modelos por ensamblado, continuan siendo mejor los modelos de ensamblado
#   Podemos aumentar a 4 grupos, juntando xgboost, gbm, bagging y rf
unipredi_3 <- cbind(pred_logistica,pred_avnnet,pred_bagging,pred_random_forest,
pred_gradient_boosting,pred_xgboost,pred_svm_lineal,pred_svm_poly, pred_svm_rbf)
#  Eliminamos columnas duplicadas
unipredi_3 <- unipredi_3[, !duplicated(colnames(unipredi_3))]
unipredi_3[, "xgboost-gbm-bagging-rf"] <- (unipredi_3[, "xgboost"] + unipredi_3[, "gbm"] + unipredi_3[, "bagging"] + unipredi_3[, "rf"]) / 4
medias_final <- rbind(
medias0[medias0$modelo %in% c("logi", "avnnet", "bagging", "rf", "gbm", "xgboost", "svmlin", "svmpoly", "svmrbf",
"bagging-rf", "rf-xgboost"), ],
medias1[medias1$modelo %in% c("bagging-rf-gbm", "bagging-rf-xgboost"), ]
)
# Cambio a Yes, No, todas las predicciones
repeticiones<-nlevels(factor(unipredi_3$Rep))
unipredi_3$Rep<-as.factor(unipredi_3$Rep)
unipredi_3$Rep<-as.numeric(unipredi_3$Rep)
dput(names(unipredi_3))
listado           <- c("xgboost-gbm-bagging-rf")
medias2<-data.frame(c())
for (prediccion in listado)
{
unipredi_3$proba<-unipredi_3[,prediccion]
unipredi_3[,prediccion]<-ifelse(unipredi_3[,prediccion]>0.5,"Yes","No")
for (repe in 1:repeticiones)
{
paso <- unipredi_3[(unipredi_3$Rep==repe),]
pre<-factor(paso[,prediccion])
archi<-paso[,c("proba","obs")]
archi<-archi[order(archi$proba),]
obs<-paso[,c("obs")]
tasa=1-tasafallos(pre,obs)
t<-as.data.frame(tasa)
t$modelo<-prediccion
auc<-suppressMessages(auc(archi$obs,archi$proba))
t$auc<-auc
medias2<-rbind(medias2,t)
}
}
medias_final <- rbind(medias_final, medias2)
medias_final$tipo <- c(rep("Original", 90), rep("Ensamblados (2)", 20),
rep("Ensamblado (3)", 20), rep("Ensamblado (4)", 10))
names(medias_final)
medias_final <- rbind(medias_final[, -4], medias2)
medias_final$tipo <- c(rep("Original", 90), rep("Ensamblados (2)", 20),
rep("Ensamblado (3)", 20), rep("Ensamblado (4)", 10))
medias_final$modelo <- with(medias_final,
reorder(modelo,tasa, mean))
ggplot(medias_final, aes(x = modelo, y = tasa, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
medias_final$modelo <- with(medias_final,
reorder(modelo,auc, mean))
ggplot(medias_final, aes(x = modelo, y = auc, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
medias_final
unipredi_4 <- cbind(pred_logistica,pred_avnnet,pred_bagging,pred_random_forest,
pred_gradient_boosting,pred_xgboost,pred_svm_lineal,pred_svm_poly, pred_svm_rbf)
#  Eliminamos columnas duplicadas
unipredi_4 <- unipredi_4[, !duplicated(colnames(unipredi_4))]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf")
for(modelos in combn(modelos, m=2, simplify = FALSE)) {
nombre_modelo  <- paste0("logi-",modelos[1])
print(nombre_modelo)
unipredi_4[, nombre_modelo] <- (unipredi_4[, modelos[1]] + unipredi_4[, "logi"]) / 2
}
unipredi_4[, modelos[1]]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf")
for(modelos in combn(modelos, m=2, simplify = FALSE)) {
nombre_modelo  <- paste0("logi-",modelos[1])
print(nombre_modelo)
unipredi_4[, nombre_modelo] <- (unipredi_4[, modelos[1]] * 0.8 + unipredi_4[, "logi"] * 0.2)
}
unipredi_4[, nombre_modelo]
#-- Sin embargo, tenemos una posibilidad de poder mejorar los modelos de regresion logistica
#   Asignando pesos a las predicciones
medias_final_2 <- medias_final
repeticiones<-nlevels(factor(unipredi_4$Rep))
unipredi_4$Rep<-as.factor(unipredi_4$Rep)
unipredi_4$Rep<-as.numeric(unipredi_4$Rep)
dput(names(unipredi_4))
unipredi_4 <- cbind(pred_logistica,pred_avnnet,pred_bagging,pred_random_forest,
pred_gradient_boosting,pred_xgboost,pred_svm_lineal,pred_svm_poly, pred_svm_rbf)
#  Eliminamos columnas duplicadas
unipredi_4 <- unipredi_4[, !duplicated(colnames(unipredi_4))]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf", "svmlin", "svmpoly")
for(modelos in combn(modelos, m=2, simplify = FALSE)) {
nombre_modelo  <- paste0("logi-",modelos[1])
print(nombre_modelo)
unipredi_4[, nombre_modelo] <- (unipredi_4[, modelos[1]] * 0.8 + unipredi_4[, "logi"] * 0.2)
}
# Cambio a Yes, No, todas las predicciones
repeticiones<-nlevels(factor(unipredi_4$Rep))
unipredi_4$Rep<-as.factor(unipredi_4$Rep)
unipredi_4$Rep<-as.numeric(unipredi_4$Rep)
dput(names(unipredi_4))
combn(modelos, m=2, simplify = FALSE)
unipredi_4 <- cbind(pred_logistica,pred_avnnet,pred_bagging,pred_random_forest,
pred_gradient_boosting,pred_xgboost,pred_svm_lineal,pred_svm_poly, pred_svm_rbf)
#  Eliminamos columnas duplicadas
unipredi_4 <- unipredi_4[, !duplicated(colnames(unipredi_4))]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf", "svmlin", "svmpoly")
for(modelos in modelos) {
nombre_modelo  <- paste0("logi-",modelo)
print(nombre_modelo)
unipredi_4[, nombre_modelo] <- (unipredi_4[, modelo] * 0.8 + unipredi_4[, "logi"] * 0.2)
}
# Cambio a Yes, No, todas las predicciones
repeticiones<-nlevels(factor(unipredi_4$Rep))
unipredi_4$Rep<-as.factor(unipredi_4$Rep)
unipredi_4$Rep<-as.numeric(unipredi_4$Rep)
dput(names(unipredi_4))
unipredi_4 <- cbind(pred_logistica,pred_avnnet,pred_bagging,pred_random_forest,
pred_gradient_boosting,pred_xgboost,pred_svm_lineal,pred_svm_poly, pred_svm_rbf)
#  Eliminamos columnas duplicadas
unipredi_4 <- unipredi_4[, !duplicated(colnames(unipredi_4))]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf", "svmlin", "svmpoly")
for(modelos in modelos) {
nombre_modelo  <- paste0("logi-",modelo)
print(nombre_modelo)
unipredi_4[, nombre_modelo] <- (unipredi_4[, modelo] * 0.8 + unipredi_4[, "logi"] * 0.2)
}
unipredi_4 <- cbind(pred_logistica,pred_avnnet,pred_bagging,pred_random_forest,
pred_gradient_boosting,pred_xgboost,pred_svm_lineal,pred_svm_poly, pred_svm_rbf)
#  Eliminamos columnas duplicadas
unipredi_4 <- unipredi_4[, !duplicated(colnames(unipredi_4))]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf", "svmlin", "svmpoly")
for(modelo in modelos) {
nombre_modelo  <- paste0("logi-",modelo)
print(nombre_modelo)
unipredi_4[, nombre_modelo] <- (unipredi_4[, modelo] * 0.8 + unipredi_4[, "logi"] * 0.2)
}
# Cambio a Yes, No, todas las predicciones
repeticiones<-nlevels(factor(unipredi_4$Rep))
unipredi_4$Rep<-as.factor(unipredi_4$Rep)
unipredi_4$Rep<-as.numeric(unipredi_4$Rep)
dput(names(unipredi_4))
listado           <- c("logi-avnnet", "logi-bagging", "logi-rf", "logi-gbm", "logi-xgboost",
"logi-svmrbf", "logi-svmlin", "logi-svmpoly")
medias3<-data.frame(c())
for (prediccion in listado)
{
unipredi_4$proba<-unipredi_4[,prediccion]
unipredi_4[,prediccion]<-ifelse(unipredi_4[,prediccion]>0.5,"Yes","No")
for (repe in 1:repeticiones)
{
paso <- unipredi_4[(unipredi_4$Rep==repe),]
pre<-factor(paso[,prediccion])
archi<-paso[,c("proba","obs")]
archi<-archi[order(archi$proba),]
obs<-paso[,c("obs")]
tasa=1-tasafallos(pre,obs)
t<-as.data.frame(tasa)
t$modelo<-prediccion
auc<-suppressMessages(auc(archi$obs,archi$proba))
t$auc<-auc
medias3<-rbind(medias3,t)
}
}
medias_final_2 <- rbind(medias_final_2[, -4], medias3)
medias_final_2
90+20+20+10
220-140
medias_final$tipo <- c(rep("Original", 90), rep("Ensamblados (2)", 20),
rep("Ensamblado (3)", 20), rep("Ensamblado (4)", 10),
rep("Weighted Average", 80))
medias_final_2$tipo <- c(rep("Original", 90), rep("Ensamblados (2)", 20),
rep("Ensamblado (3)", 20), rep("Ensamblado (4)", 10),
rep("Weighted Average", 80))
medias_final_2$modelo <- with(medias_final_2,
reorder(modelo,tasa, mean))
ggplot(medias_final_2, aes(x = modelo, y = tasa, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
ggplot(medias_final_2, aes(x = modelo, y = auc, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
medias_final_2$modelo <- with(medias_final_2,
reorder(modelo,auc, mean))
ggplot(medias_final_2, aes(x = modelo, y = auc, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
#  Eliminamos columnas duplicadas
unipredi_5 <- unipredi_5[, !duplicated(colnames(unipredi_5))]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf", "svmlin", "svmpoly")
for(modelo in combn(modelos, m=2, simplify = FALSE)) {
nombre_modelo  <- paste0("logi-",modelo)
print(nombre_modelo)
unipredi_5[, nombre_modelo] <- unipredi_5[, modelos[1]] * 0.35 + unipredi_5[, modelos[2]] * 0.35 + unipredi_5[, "logi"] * 0.2
}
# Cambio a Yes, No, todas las predicciones
repeticiones<-nlevels(factor(unipredi_5$Rep))
unipredi_5$Rep<-as.factor(unipredi_5$Rep)
unipredi_5$Rep<-as.numeric(unipredi_5$Rep)
#-- ¿Y empleando tres modelos?
unipredi_5 <- cbind(pred_logistica,pred_avnnet,pred_bagging,pred_random_forest,
pred_gradient_boosting,pred_xgboost,pred_svm_lineal,pred_svm_poly, pred_svm_rbf)
#  Eliminamos columnas duplicadas
unipredi_5 <- unipredi_5[, !duplicated(colnames(unipredi_5))]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf", "svmlin", "svmpoly")
for(modelo in combn(modelos, m=2, simplify = FALSE)) {
nombre_modelo  <- paste0("logi-",modelo)
print(nombre_modelo)
unipredi_5[, nombre_modelo] <- unipredi_5[, modelos[1]] * 0.35 + unipredi_5[, modelos[2]] * 0.35 + unipredi_5[, "logi"] * 0.2
}
# Cambio a Yes, No, todas las predicciones
repeticiones<-nlevels(factor(unipredi_5$Rep))
unipredi_5$Rep<-as.factor(unipredi_5$Rep)
unipredi_5$Rep<-as.numeric(unipredi_5$Rep)
dput(names(unipredi_5))
#-- ¿Y empleando tres modelos?
unipredi_5 <- cbind(pred_logistica,pred_avnnet,pred_bagging,pred_random_forest,
pred_gradient_boosting,pred_xgboost,pred_svm_lineal,pred_svm_poly, pred_svm_rbf)
#  Eliminamos columnas duplicadas
unipredi_5 <- unipredi_5[, !duplicated(colnames(unipredi_5))]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf", "svmlin", "svmpoly")
for(modelo in combn(modelos, m=2, simplify = FALSE)) {
nombre_modelo  <- paste0("logi-",modelos[1],"-",modelos[2])
print(nombre_modelo)
unipredi_5[, nombre_modelo] <- unipredi_5[, modelos[1]] * 0.35 + unipredi_5[, modelos[2]] * 0.35 + unipredi_5[, "logi"] * 0.2
}
# Cambio a Yes, No, todas las predicciones
repeticiones<-nlevels(factor(unipredi_5$Rep))
unipredi_5$Rep<-as.factor(unipredi_5$Rep)
unipredi_5$Rep<-as.numeric(unipredi_5$Rep)
dput(names(unipredi_5))
unipredi_5 <- cbind(pred_logistica,pred_avnnet,pred_bagging,pred_random_forest,
pred_gradient_boosting,pred_xgboost,pred_svm_lineal,pred_svm_poly, pred_svm_rbf)
#  Eliminamos columnas duplicadas
unipredi_5 <- unipredi_5[, !duplicated(colnames(unipredi_5))]
modelos  <- c("avnnet", "bagging", "rf", "gbm", "xgboost", "svmrbf", "svmlin", "svmpoly")
for(modelo in combn(modelos, m=2, simplify = FALSE)) {
nombre_modelo  <- paste0("logi-",modelo[1],"-",modelo[2])
print(nombre_modelo)
unipredi_5[, nombre_modelo] <- unipredi_5[, modelo[1]] * 0.35 + unipredi_5[, modelo[2]] * 0.35 + unipredi_5[, "logi"] * 0.2
}
# Cambio a Yes, No, todas las predicciones
repeticiones<-nlevels(factor(unipredi_5$Rep))
unipredi_5$Rep<-as.factor(unipredi_5$Rep)
unipredi_5$Rep<-as.numeric(unipredi_5$Rep)
dput(names(unipredi_5))
medias4<-data.frame(c())
medias4<-data.frame(c())
for (prediccion in listado)
{
unipredi_5$proba<-unipredi_5[,prediccion]
unipredi_5[,prediccion]<-ifelse(unipredi_5[,prediccion]>0.5,"Yes","No")
for (repe in 1:repeticiones)
{
paso <- unipredi_5[(unipredi_5$Rep==repe),]
pre<-factor(paso[,prediccion])
archi<-paso[,c("proba","obs")]
archi<-archi[order(archi$proba),]
obs<-paso[,c("obs")]
tasa=1-tasafallos(pre,obs)
t<-as.data.frame(tasa)
t$modelo<-prediccion
auc<-suppressMessages(auc(archi$obs,archi$proba))
t$auc<-auc
medias4<-rbind(medias4,t)
}
}
listado           <- c("logi-avnnet-bagging",
"logi-avnnet-rf", "logi-avnnet-gbm", "logi-avnnet-xgboost", "logi-avnnet-svmrbf",
"logi-avnnet-svmlin", "logi-avnnet-svmpoly", "logi-bagging-rf",
"logi-bagging-gbm", "logi-bagging-xgboost", "logi-bagging-svmrbf",
"logi-bagging-svmlin", "logi-bagging-svmpoly", "logi-rf-gbm",
"logi-rf-xgboost", "logi-rf-svmrbf", "logi-rf-svmlin", "logi-rf-svmpoly",
"logi-gbm-xgboost", "logi-gbm-svmrbf", "logi-gbm-svmlin", "logi-gbm-svmpoly",
"logi-xgboost-svmrbf", "logi-xgboost-svmlin", "logi-xgboost-svmpoly",
"logi-svmrbf-svmlin", "logi-svmrbf-svmpoly", "logi-svmlin-svmpoly")
medias4<-data.frame(c())
for (prediccion in listado)
{
unipredi_5$proba<-unipredi_5[,prediccion]
unipredi_5[,prediccion]<-ifelse(unipredi_5[,prediccion]>0.5,"Yes","No")
for (repe in 1:repeticiones)
{
paso <- unipredi_5[(unipredi_5$Rep==repe),]
pre<-factor(paso[,prediccion])
archi<-paso[,c("proba","obs")]
archi<-archi[order(archi$proba),]
obs<-paso[,c("obs")]
tasa=1-tasafallos(pre,obs)
t<-as.data.frame(tasa)
t$modelo<-prediccion
auc<-suppressMessages(auc(archi$obs,archi$proba))
t$auc<-auc
medias4<-rbind(medias4,t)
}
}
medias_final_1
medias_final
medias4
medias_final_3 <- rbind(medias_final[, -4], medias4)
medias_final_3
medias_final_3$tipo <- c(rep("Original", 90), rep("Ensamblados (2)", 20),
rep("Ensamblado (3)", 20), rep("Ensamblado (4)", 10),
rep("Weighted Average", 280))
medias_final_3$modelo <- with(medias_final_3,
reorder(modelo,tasa, mean))
ggplot(medias_final_3, aes(x = modelo, y = tasa, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
ggplot(medias_final_3, aes(x = modelo, y = auc, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
medias_final_3$modelo <- with(medias_final_3,
reorder(modelo,tasa, mean))
ggplot(medias_final_3, aes(x = modelo, y = tasa, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
medias_final_3$modelo <- with(medias_final_3,
reorder(modelo,auc, mean))
ggplot(medias_final_3, aes(x = modelo, y = auc, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
medias_final$modelo <- with(medias_final,
reorder(modelo,tasa, mean))
ggplot(medias_final[!medias_final$tipo %in% c("Original"), ], aes(x = modelo, y = tasa, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
medias_final$modelo <- with(medias_final,
reorder(modelo,auc, mean))
ggplot(medias_final[!medias_final$tipo %in% c("Original"), ], aes(x = modelo, y = auc, col = tipo)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (modelos originales + ensamblados)") +
theme(axis.text.x = element_text(angle = 45))
medias_final
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Ensamblado.RData")
#--- Lectura dataset depurado
surgical_dataset <- fread("./data/surgical_dataset_final.csv", data.table = FALSE)
surgical_dataset$target <- as.factor(surgical_dataset$target)
# Separamos variable objetivo del resto
target <- "target"
#-- Modelo 2
var_modelo1 <- c("mortality_rsi", "ccsMort30Rate", "bmi", "month.8", "Age")
var_modelo2 <- c("mortality_rsi", "bmi", "month.8", "Age")
h2o.init()
#--- Librerias
suppressPackageStartupMessages({
library(data.table)    # Lectura de ficheros mucho mas rapido que read.csv
library(parallel)      # Paralelizacion de funciones (I)
library(doParallel)    # Paralelizacion de funciones (II)
library(caret)         # Recursive Feature Elimination
library(readxl)        # Lectura ficheros .xlsx
library(DescTools)     # Reordenacion de variales categoricas
library(ggrepel)       # Labels ggplot2
library(stringi)       # Tratamiento de strings
library(corrplot)      # Matriz de correlacion (grafico)
library(h2o)           # AutoML
})
h2o.init()
#-- Covertimos el dataset a un objeto h2o
surgical_dataset_h <- as.h2o(surgical_dataset)
names(surgical_dataset)
aml_1 <- h2o.automl(x = var_modelo1, y = target,
training_frame = surgical_dataset_h, nfolds = 5,
seed = 1234, balance_classes = TRUE, keep_cross_validation_predictions = TRUE,
max_models = 20)
lb_1 <- aml_1@leaderboard
print(lb, n = nrow(lb_1))
print(lb_1, n = nrow(lb_1))
aml_2 <- h2o.automl(x = var_modelo2, y = target,
training_frame = surgical_dataset_h, nfolds = 5, seed = 1234,
balance_classes = TRUE, keep_cross_validation_predictions = TRUE,
max_models = 20)
leader_1 <- aml_1@leader
leader_1
lb_2 <- aml_2@leaderboard
print(lb_2, n = nrow(lb_2))
print(lb_1, n = nrow(lb_1))
leader_1@parameters
surgical_dataset[unique(surgical_dataset) == 2, ]
unique(surgical_dataset) == 2
apply(surgical_dataset, 0, function(x) unique(x))
apply(surgical_dataset, 1, function(x) unique(x))
apply(surgical_dataset, 1, function(x) unique(x) == 2)
apply(surgical_dataset, 1, function(x) print(unique(x) == 2))
apply(surgical_dataset, 0, function(x) print(unique(x) == 2))
surgical_dataset[sapply(surgical_dataset, function(x) length(unique(x))==2)]
table(surgical_dataset[sapply(surgical_dataset, function(x) length(unique(x))==2)])
table(surgical_dataset[sapply(surgical_dataset, function(x) length(unique(x))==2), ])
table(surgical_dataset[sapply(surgical_dataset, function(x) length(unique(x))==2)])
surgical_dataset[sapply(surgical_dataset, function(x) length(unique(x))==2)] %>% tally()
library(dplyr)
surgical_dataset[sapply(surgical_dataset, function(x) length(unique(x))==2)] %>% tally()
surgical_dataset[sapply(surgical_dataset, function(x) length(unique(x))==2)] %>% map(function(x) table(x))
surgical_dataset[sapply(surgical_dataset, function(x) length(unique(x))==2)] %>% purr::map(function(x) table(x))
surgical_dataset[sapply(surgical_dataset, function(x) length(unique(x))==2)] %>% purrr::map(function(x) table(x))
lb_2 <- aml_2@leaderboard
print(lb_2, n = nrow(lb_2))
leader_2 <- aml_1@leader
leader_2 <- aml_2@leader
leader_2
leader_2@parameters
parameters_1 <- leader_1@parameters
parameters_1
parameters_2 <- leader_2@parameters
parameters_2
# En ambos casos, observamos que el mejor modelo corresponde con un modelo de Gradient Boosting
# Analicemos las diferencias entre sus parametros
parameters_1[lapply(names(Lobs), function(x) !identical(parameters_1[[x]], parameters_2[[x]]))]
# En ambos casos, observamos que el mejor modelo corresponde con un modelo de Gradient Boosting
# Analicemos las diferencias entre sus parametros
parameters_1[lapply(names(parameters_1), function(x) !identical(parameters_1[[x]], parameters_2[[x]]))]
# En ambos casos, observamos que el mejor modelo corresponde con un modelo de Gradient Boosting
# Analicemos las diferencias entre sus parametros
parameters_1[sapply(names(parameters_1), function(x) !identical(parameters_1[[x]], parameters_2[[x]]))]
parameters_1
parameters_1
parameters_2
parameters_1
parameters_2
print(lb_1, n = nrow(lb_1))
#   Nos guardamos los modelos
modelo1 <- h2o.getModel("GBM_5_AutoML_20210422_174850")
print(lb_2, n = nrow(lb_2))
modelo2 <- h2o.getModel("GBM_5_AutoML_20210422_182719")
leader_2
leader_1
parameters_1
print(lb_1, n = nrow(lb_1))
View(leader_1)
gbm1 <- h2o.gbm(
x = var_modelo1, y = target, training_frame = surgical_dataset, seed = 1234,
sample_rate = 0.8, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8,
max_depth = 15, min_rows = 100, stopping_tolerance = 0.01306994,
score_tree_interval = 5, ntrees = 82, nfolds = 5
)
gbm2 <- h2o.gbm(
x = var_modelo1, y = target, training_frame = surgical_dataset, seed =
sample_rate = 0.8, col_sample_rate = 0.8,
col_sample_rate_per_tree = 0.8, max_depth = 15, min_rows = 100, stopping_tolerance = 0.01306994,
score_tree_interval = 5, ntrees = 72, nfolds = 5
)
gbm1 <- h2o.gbm(
x = var_modelo1, y = target, training_frame = surgical_dataset_h, seed = 1234,
sample_rate = 0.8, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8,
max_depth = 15, min_rows = 100, stopping_tolerance = 0.01306994,
score_tree_interval = 5, ntrees = 82, nfolds = 5
)
gbm2 <- h2o.gbm(
x = var_modelo1, y = target, training_frame = surgical_dataset_h, seed = 1234,
sample_rate = 0.8, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8,
max_depth = 15, min_rows = 100, stopping_tolerance = 0.01306994,
score_tree_interval = 5, ntrees = 72, nfolds = 5
)
gbm1
gbm1 <- h2o.gbm(
x = var_modelo1, y = target, training_frame = surgical_dataset_h, seed = 1234,
sample_rate = 0.8, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8,
max_depth = 15, min_rows = 100, stopping_tolerance = 0.01306994,
score_tree_interval = 5, ntrees = 82, nfolds = 5, keep_cross_validation_predictions = TRUE
)
gbm2 <- h2o.gbm(
x = var_modelo1, y = target, training_frame = surgical_dataset_h, seed = 1234,
sample_rate = 0.8, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8,
max_depth = 15, min_rows = 100, stopping_tolerance = 0.01306994,
score_tree_interval = 5, ntrees = 72, nfolds = 5, keep_cross_validation_predictions = TRUE
)
gbm1
gbm1
gbm2
h2o.shutdown(prompt = FALSE)
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/H2O.RData")
