scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
View(union_bagging_modelo1)
View(union_bagging_modelo1)
union_bagging_modelo1[c(1:30),]
union_bagging_modelo1[c(1:40),]
union_bagging_modelo1[c(1:41),]
rbind(union_bagging_modelo1[c(1:40), c("tasa", "auc", "modelo")], union_bagging_modelo1_bis)
union_bagging_modelo1 <- rbind(union_bagging_modelo1[c(1:40), c("tasa", "auc", "modelo")], union_bagging_modelo1_bis)
union_bagging_modelo1$rep <- c(rep("5", 40), rep("10", 80))
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,tasa, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = tasa, col = rep)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,auc, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = auc, col = rep)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
#           sampsize,
# Autor: Alberto Fernandez Hernandez
#--- Librerias
suppressPackageStartupMessages({
library(data.table)    # Lectura de ficheros mucho mas rapido que read.csv
library(parallel)      # Paralelizacion de funciones (I)
library(doParallel)    # Paralelizacion de funciones (II)
library(caret)         # Recursive Feature Elimination
library(randomForest)  # Seleccion del numero de arboles
library(readxl)        # Lectura de ficheros Excel
source("./librerias/librerias_propias.R")
})
#--- Creamos el cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
nodesizes.1 <- list(20)
sampsizes.1 <- list(1000, 1500)
bagging_modelo1_2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = 1800, grupos = 5, repe = 10)
# Nodesize 30: 1500-2000 parece ser una buena opcion (en ambos casos la varianza AUC parece ser muy similar), aunque
# la varianza con respecto a la tasa de fallos es ligeramente menor con 2000 observaciones
nodesizes.1 <- list(30)
sampsizes.1 <- list(1500, 2000)
bagging_modelo1_3 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = 1800, grupos = 5, repe = 10)
# Nodesize 40: en este caso, 1500-2000-2500 parecen ser alternativas al conjunto total de observaciones. Sin embargo,
# con 2000 y 2500 observaciones la varianza en relacion tanto al AUC como la tasa de fallos es menor en comparacion al modelo
# con solo 1000 observaciones
nodesizes.1 <- list(40)
sampsizes.1 <- list(1500, 2000, 2500)
bagging_modelo1_4 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = 1800, grupos = 5, repe = 10)
# ¿Merece la pena aumentar el nodesize a 50? ¿O disminuirlo a 10?
nodesizes.1 <- list(50)
sampsizes.1 <- list(3000)
bagging_modelo1_5 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = 1800, grupos = 5, repe = 10)
union_bagging_modelo1_bis <- rbind(
bagging_modelo1_2[bagging_modelo1_2$modelo == "20+1000", ],
bagging_modelo1_2[bagging_modelo1_2$modelo == "20+1500", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "30+1500", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "30+2000", ],
bagging_modelo1_4[bagging_modelo1_4$modelo == "40+1500", ],
bagging_modelo1_4[bagging_modelo1_4$modelo == "40+2000", ],
bagging_modelo1_4[bagging_modelo1_4$modelo == "40+2500", ],
bagging_modelo1_5[bagging_modelo1_5$modelo == "50+3000", ]
)
union_bagging_modelo1 <- rbind(union_bagging_modelo1[c(1:40), c("modelo", "tasa", "auc")], union_bagging_modelo1_bis)
union_bagging_modelo1$rep <- c(rep("5", 40), rep("10", 80))
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,tasa, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = tasa, col = rep)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,auc, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = auc, col = rep)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
suppressPackageStartupMessages({
library(data.table)    # Lectura de ficheros mucho mas rapido que read.csv
library(parallel)      # Paralelizacion de funciones (I)
library(doParallel)    # Paralelizacion de funciones (II)
library(caret)         # Recursive Feature Elimination
library(randomForest)  # Seleccion del numero de arboles
library(readxl)        # Lectura de ficheros Excel
source("./librerias/librerias_propias.R")
})
#--- Creamos el cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
bagging_modelo2_2
bagging_modelo1_3
nodesizes.2 <- list(20)
sampsizes.2 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo2_2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 5)
1000/5854
nodesizes.2 <- list(30)
sampsizes.2 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo2_3 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 5)
nodesizes.2 <- list(40)
sampsizes.2 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo2_4 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 5)
nodesizes.2 <- list(50)
sampsizes.2 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo2_5 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 5)
nodesizes.2 <- list(10)
sampsizes.2 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo2_5 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 5)
union_bagging_modelo2 <- rbind(
bagging_modelo2_2[bagging_modelo2_2$modelo == "20+1000", ],
bagging_modelo2_3[bagging_modelo2_3$modelo == "30+1500", ],
bagging_modelo2_3[bagging_modelo2_3$modelo == "30+2000", ],
bagging_modelo2_4[bagging_modelo2_4$modelo == "40+1000", ],
bagging_modelo2_4[bagging_modelo2_4$modelo == "40+1500", ],
bagging_modelo2_4[bagging_modelo2_4$modelo == "40+2000", ],
bagging_modelo2_5[bagging_modelo2_5$modelo == "10+1000", ],
bagging_modelo2_5[bagging_modelo2_5$modelo == "10+1500", ]
)
union_bagging_modelo2$modelo <- with(union_bagging_modelo2,
reorder(modelo,tasa, mean))
ggplot(union_bagging_modelo2, aes(x = modelo, y = tasa, col = rep)) +
geom_boxplot(fill = "#4271AE", colour = "#1F3552", alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggsave("./charts/bagging/03_comparacion_final_tasa_modelo2_5rep.png")
union_bagging_modelo2$modelo <- with(union_bagging_modelo2,
reorder(modelo,auc, mean))
ggplot(union_bagging_modelo2, aes(x = modelo, y = auc, col = rep)) +
geom_boxplot(fill = "#4271AE", colour = "#1F3552", alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
ggsave("./charts/bagging/03_comparacion_final_auc_modelo2_5rep.png")
2000/5854
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
nodesizes.2 <- list(20)
sampsizes.2 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo2_2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 10)
# Nodesize 30: con 1500 e incluso 2000 submuestras el modelo alcanza su AUC maximo. Por otro lado,
# el modelo con sampsize 1000, aunque presente un buen AUC (especialmente en varianza), su varianza en
# relacion a la tasa de fallo es muy elevada
nodesizes.2 <- list(30)
sampsizes.2 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo2_3 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 10)
# Nodesize 40: con 1000-1500-2000 submuestras se obtiene un mayor valor AUC
nodesizes.2 <- list(40)
sampsizes.2 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo2_4 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 10)
# A medida que aumenta el nodesize la variabilidad tanto en sesgo como en varianza disminuye
# ¿Mereceria la pena aumentar nodesize a 50? ¿O disminuir nodesize a 10?
nodesizes.2 <- list(10)
sampsizes.2 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo2_5 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 10)
union_bagging_modelo2_bis <- rbind(
bagging_modelo2_2[bagging_modelo2_2$modelo == "20+1000", ],
bagging_modelo2_3[bagging_modelo2_3$modelo == "30+1500", ],
bagging_modelo2_3[bagging_modelo2_3$modelo == "30+2000", ],
bagging_modelo2_4[bagging_modelo2_4$modelo == "40+1000", ],
bagging_modelo2_4[bagging_modelo2_4$modelo == "40+1500", ],
bagging_modelo2_4[bagging_modelo2_4$modelo == "40+2000", ],
bagging_modelo2_5[bagging_modelo2_5$modelo == "10+1000", ],
bagging_modelo2_5[bagging_modelo2_5$modelo == "10+1500", ]
)
union_bagging_modelo2 <- rbind(union_bagging_modelo2, union_bagging_modelo2_bis)
union_bagging_modelo2$rep <- c(rep("5", 45), rep("10", 90))
union_bagging_modelo2$rep <- c(rep("5", 40), rep("10", 80))
union_bagging_modelo2$modelo <- with(union_bagging_modelo2,
reorder(modelo,tasa, mean))
ggplot(union_bagging_modelo2, aes(x = modelo, y = tasa, col = rep)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggsave("./charts/bagging/bis_03_comparacion_final_tasa_modelo2_10rep.png")
union_bagging_modelo2$modelo <- with(union_bagging_modelo2,
reorder(modelo,auc, mean))
ggplot(union_bagging_modelo2, aes(x = modelo, y = auc, col = rep)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
ggsave("./charts/bagging/bis_03_comparacion_final_auc_modelo2_10rep.png")
union_bagging_modelo2
View(mostrar_err_rate)
mostrar_err_rate <- function(train.err.rate1, train.err.rate2) {
plot(train.err.rate1, col = 'red', type = 'p',
main = 'Error rate by nº trees', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
points(train.err.rate2, col = 'blue')
legend("topright", legend = c("OOB: MODELO 2","OOB: MODELO 1") ,
col = c('red', 'blue') , bty = "n", horiz = FALSE,
lty=1, cex = 0.75)
}
mostrar_err_rate(rfbis.2$err.rate[, 1], rfbis.1$err.rate[, 1])
mostrar_err_rate <- function(train.err.rate1, train.err.rate2) {
plot(train.err.rate1, col = 'red', type = 'l',
main = 'Error rate by nº trees', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
lines(train.err.rate2, col = 'blue')
legend("topright", legend = c("OOB: MODELO 2","OOB: MODELO 1") ,
col = c('red', 'blue') , bty = "n", horiz = FALSE,
lty=1, cex = 0.75)
}
mostrar_err_rate(rfbis.2$err.rate[, 1], rfbis.1$err.rate[, 1])
mostrar_err_rate(rfbis.2$err.rate[c(0:3000), 1], rfbis.1$err.rate[c(0:3000), 1])
surgical_test_data <- fread("./data/surgical_test_data.csv", data.table = FALSE)
names(surgical_test_data)[35] <- "target"
surgical_test_data$target     <- as.factor(surgical_test_data$target)
# Aplico caret y construyo modelos finales
control <- trainControl(method = "repeatedcv",number=5,repeats=10,
savePredictions = "all",classProbs=TRUE)
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
n.trees.2
baggin_1_800 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 20,
sampsizes = 1000, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
baggin_1_1600 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 20,
sampsizes = 1000, mtry = mtry.1,
ntree = 1600, grupos = 5, repe = 10)
baggin_2_800 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = 30,
sampsizes = 2000, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 10)
baggin_2_1600 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = 30,
sampsizes = 2000, mtry = mtry.2,
ntree = 1600, grupos = 5, repe = 10)
bagging <- rbind(baggin_1_800, baggin_1_1600, baggin_2_800, baggin_2_1600)
bagging$ntrees <- c(rep("800", 10), rep("1600", 10), rep("800", 10), rep("1600", 10))
bagging
ggplot(baggin_1, aes(x = modelo, y = ntrees)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggplot(bagging, aes(x = modelo, y = ntrees)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggplot(bagging, aes(x = modelo, y = tasa, col = ntrees)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggplot(bagging, aes(x = modelo, y = tasa, col = ntrees)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggsave('./charts/bagging/03_comparativa_800_1600_tasa.jpeg')
ggplot(bagging, aes(x = modelo, y = auc, col = ntrees)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
ggsave('./charts/bagging/03_comparativa_800_1600_auc.jpeg')
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
surgical_test_data <- fread("./data/surgical_test_data.csv", data.table = FALSE)
names(surgical_test_data)[35] <- "target"
surgical_test_data$target     <- as.factor(surgical_test_data$target)
# Aplico caret y construyo modelos finales
control <- trainControl(method = "repeatedcv",number=5,repeats=10,
savePredictions = "all",classProbs=TRUE)
baggin_1_800 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 20,
sampsizes = 1000, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
baggin_1_1700 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 20,
sampsizes = 1000, mtry = mtry.1,
ntree = 1700, grupos = 5, repe = 10)
baggin_2_800 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = 30,
sampsizes = 2000, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 10)
baggin_2_1700 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = 30,
sampsizes = 2000, mtry = mtry.2,
ntree = 1700, grupos = 5, repe = 10)
bagging <- rbind(baggin_1_800, baggin_1_1700, baggin_2_800, baggin_2_1700)
bagging$ntrees <- c(rep("800", 10), rep("1700", 10), rep("800", 10), rep("1700", 10))
ggplot(bagging, aes(x = modelo, y = tasa, col = ntrees)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggsave('./charts/bagging/03_comparativa_800_1700_tasa.jpeg')
ggplot(bagging, aes(x = modelo, y = auc, col = ntrees)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
ggsave('./charts/bagging/03_comparativa_800_1700_auc.jpeg')
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
surgical_test_data <- fread("./data/surgical_test_data.csv", data.table = FALSE)
names(surgical_test_data)[35] <- "target"
surgical_test_data$target     <- as.factor(surgical_test_data$target)
# Aplico caret y construyo modelos finales
control <- trainControl(method = "repeatedcv",number=5,repeats=10,
savePredictions = "all",classProbs=TRUE)
#-- Modelo 1
rfgrid.1 <-expand.grid(mtry=mtry.1)
set.seed(1234)
bagging_1 <- train(as.formula(paste0(target, "~" , paste0(var_modelo1, collapse = "+"))),
data=surgical_dataset, method="rf", trControl = control,tuneGrid = rfgrid.1,
nodesize = 20, sampsize = 1000, ntree = n.trees.1, replace = TRUE)
matriz_conf_1 <- matriz_confusion_predicciones(bagging_1, NULL, surgical_test_data, 0.5)
matriz_conf_1
rfgrid.2 <-expand.grid(mtry=mtry.2)
set.seed(1234)
bagging_2 <- train(as.formula(paste0(target, "~" , paste0(var_modelo2, collapse = "+"))),
data=surgical_dataset, method="rf", trControl = control,tuneGrid = rfgrid.2,
nodesize = 30, sampsize = 2000, ntree = n.trees.2, replace = TRUE)
matriz_conf_2 <- matriz_confusion_predicciones(bagging_2, NULL, surgical_test_data, 0.5)
rm(rfgrid.1)
rm(rfgrid.2)
matriz_conf_2
union_bagging_modelo2
union_bagging_modelo2
union_bagging_modelo1
modelos_actuales <- as.data.frame(read_excel("./ComparativaModelos.xlsx",
sheet = "bagging"))
modelos_actuales$tasa <- as.numeric(modelos_actuales$tasa)
modelos_actuales$auc <- as.numeric(modelos_actuales$auc)
modelos_actuales
modelos_actuales
nodesizes.final <- list(30)
sampsizes.final <- list(2000)
bagging_modelo_sin_reemp <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.final,
sampsizes = sampsizes.final, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 10, replace = FALSE)
bagging_modelo_sin_reemp$modelo <- "BAG. MODELO 2 (no reemp)"
modelos_actuales <- rbind(modelos_actuales, bagging_modelo_sin_reemp)
modelos_actuales$modelo <- with(modelos_actuales,
reorder(modelo,tasa, mean))
ggplot(modelos_actuales, aes(x = modelo, y = tasa)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggsave('./charts/comparativas/03_log_avnnet_bagging_tasa.jpeg')
modelos_actuales$modelo <- with(modelos_actuales,
reorder(modelo,auc, mean))
ggplot(modelos_actuales, aes(x = modelo, y = auc)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
ggsave('./charts/comparativas/03_log_avnnet_bagging_auc.jpeg')
stopCluster(cluster)
#---- Guardamos el fichero RData
save.image(file = "./rdata/Bagging.RData")
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/SeleccionVariables.RData")
candidato.aic
candidato.bic
View(lista.variables.aic)
View(tabla.aic)
View(tabla.bic)
intersect(candidato.aic, candidato.bic) %in% candidato.bic
intersect(candidato.aic, candidato.bic)
salida.rfe.lr
salida.rfe.lr$optVariables
ggplot(salida.rfe.lr) + ggtitle("Variable importance Logistic Regression RFE")
salida.rfe.rf
salida.rfe.rf$optVariables
ggplot(salida.rfe.rf) + ggtitle("Variable importance Random Forest RFE")
candidatos         <- list(candidato.aic, candidato.bic, candidato.rfe.lr, candidato.rfe.lr.2, candidato.rfe.rf)
nombres_candidatos <- c("LOGISTICA AIC", "LOGISTICA BIC", "RFE LR TOP 18", "RFE LR TOP 3", "RFE RF TOP 5")
union1 <- cruzada_logistica(surgical_dataset, target, candidatos, nombres_candidatos,
grupos = 5, repe = 5)
suppressPackageStartupMessages({
library(data.table)    # Lectura de ficheros mucho mas rapido que read.csv
library(parallel)      # Paralelizacion de funciones (I)
library(doParallel)    # Paralelizacion de funciones (II)
library(caret)         # Recursive Feature Elimination
library(ggplot2)       # Libreria grafica
source("./librerias/librerias_propias.R")
source("./librerias/funcion steprepetido binaria.R")
})
#--- Creamos el cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
candidatos         <- list(candidato.aic, candidato.bic, candidato.rfe.lr, candidato.rfe.lr.2, candidato.rfe.rf)
nombres_candidatos <- c("LOGISTICA AIC", "LOGISTICA BIC", "RFE LR TOP 18", "RFE LR TOP 3", "RFE RF TOP 5")
union1 <- cruzada_logistica(surgical_dataset, target, candidatos, nombres_candidatos,
grupos = 5, repe = 5)
candidato.bic.2 <- c("mortality_rsi", "ccsMort30Rate", "bmi", "month.8", "dow.0",
"Age", "moonphase.0", "baseline_osteoart")
candidatos_2         <- list(candidato.aic, candidato.bic, candidato.bic.2, candidato.rfe.lr, candidato.rfe.lr.2, candidato.rfe.rf)
nombres_candidatos_2 <- c("LOGISTICA AIC", "LOGISTICA BIC", "LOGISTICA BIC (sin asa.status)" , "RFE LR TOP 18", "RFE LR TOP 3", "RFE RF TOP 5")
union2 <- cruzada_logistica(surgical_dataset, target, candidatos_2, nombres_candidatos_2,
grupos = 5, repe = 5)
candidato.aic.2 <- c("mortality_rsi", "ccsMort30Rate", "bmi", "month.8", "baseline_charlson",
"baseline_osteoart", "moonphase.0", "Age", "dow.0", "month.0",
"ahrq_ccs")
candidatos_3         <- list(candidato.aic, candidato.aic.2, candidato.bic, candidato.bic.2,
candidato.rfe.lr, candidato.rfe.lr.2, candidato.rfe.rf)
nombres_candidatos_3 <- c("LOGISTICA AIC", "LOGISTICA AIC (sin 2 variables)" ,"LOGISTICA BIC", "LOGISTICA BIC (sin asa.status)" , "RFE LR TOP 18", "RFE LR TOP 3", "RFE RF TOP 5")
union3 <- cruzada_logistica(surgical_dataset, target, candidatos_3, nombres_candidatos_3,
grupos = 5, repe = 5)
rf_modelo_bic <- train_rf_model(surgical_dataset,
as.formula(paste0("target~", paste0(candidato.bic.2, collapse = "+"))),
mtry = c(3:8), ntree = 1000, grupos = 5, repe = 5, nodesize = 10,
seed = 1234)
View(train_rf_model)
train_rf_model <- function(dataset, formula, mtry, ntree, grupos, repe,
nodesize, seed) {
set.seed(seed)
rfgrid <- expand.grid(mtry=c(mtry))
control <- trainControl(method = "repeatedcv",number=grupos, repeats=repe,
savePredictions = "all",classProbs = TRUE)
rf<- train(formula,data=dataset,
method="rf",trControl=control,tuneGrid=rfgrid,
linout = FALSE,ntree=ntree,nodesize=nodesize,
replace=TRUE, importance=TRUE)
return(rf)
}
rf_modelo_bic <- train_rf_model(surgical_dataset,
as.formula(paste0("target~", paste0(candidato.bic.2, collapse = "+"))),
mtry = c(3:8), ntree = 1000, grupos = 5, repe = 5, nodesize = 10,
seed = 1234)
show_vars_importance(rf_modelo_bic, "Importancia variables Random Forest (modelo BIC)")
candidato.bic.3 <- c("mortality_rsi", "ccsMort30Rate", "bmi", "month.8", "Age")
candidatos_4         <- list(candidato.aic, candidato.aic.2, candidato.bic, candidato.bic.2, candidato.bic.3,
candidato.rfe.lr, candidato.rfe.lr.2, candidato.rfe.rf)
nombres_candidatos_4 <- c("LOGISTICA AIC", "LOGISTICA AIC (sin 2 variables)" ,"LOGISTICA BIC", "LOGISTICA BIC (sin asa.status)" ,
"LOGISTICA BIC (TOP 5)", "RFE LR TOP 18", "RFE LR TOP 3", "RFE RF TOP 5")
union4 <- cruzada_logistica(surgical_dataset, target, candidatos_4, nombres_candidatos_4,
grupos = 5, repe = 5)
rf_modelo_bic
rf_modelo_bic_2 <- train_rf_model(surgical_dataset,
as.formula(paste0("target~", paste0(candidato.aic.2, collapse = "+"))),
mtry = c(3:8), ntree = 1000, grupos = 5, repe = 5, nodesize = 10,
seed = 1234)
rf_modelo_bic_2 <- train_rf_model(surgical_dataset,
as.formula(paste0("target~", paste0(candidato.aic.2, collapse = "+"))),
mtry = c(3:11), ntree = 1000, grupos = 5, repe = 5, nodesize = 10,
seed = 1234)
rf_modelo_aic_2 <- train_rf_model(surgical_dataset,
as.formula(paste0("target~", paste0(candidato.aic.2, collapse = "+"))),
mtry = c(3:11), ntree = 1000, grupos = 5, repe = 5, nodesize = 10,
seed = 1234)
rf_modelo_aic_2
show_vars_importance(rf_modelo_aic_2, "Importancia variables Random Forest (modelo AIC)")
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/SeleccionVariables.RData")
