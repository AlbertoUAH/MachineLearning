"citric_acid")
# Se transforma la varible dependiente a Yes y No
data_norm$style <- ifelse(data_norm$style == 1, "Yes", "No")
data_norm$style <- as.factor(data_norm$style)
medias1<-cruzadalogistica(data=data_norm,
vardep="style",
listconti= variables,
listclass=c(""),
grupos=4,
sinicio=1234,
repe=5)
sapply(data_norm, class)
unique(data_norm$style)
# Funcion para normalizar
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_norm <- as.data.frame(lapply(dataset[,-c(12, 13)], normalize))
data_norm$quality <- dataset$quality
data_norm$style <- dataset$style
library(dummies)
data_norm$quality_Cool <- dummy(data_norm$quality)[,1]
data_norm$quality_Poor <- dummy(data_norm$quality)[,2]
data_norm$quality <- NULL
# Se pasa la variable objetivo a factor.
data_norm$style <- as.factor(data_norm$style)
dput(names(data_norm))
c("fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar",
"chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide", "density",
"pH", "sulphates", "alcohol", "style", "quality_Cool", "quality_Poor"
)
full<-glm(style~.,data=data_norm, family = binomial(link="logit"))
null<-glm(style~1,data=data_norm, family = binomial(link="logit"))
library(MASS)
selec1<-stepAIC(null,scope=list(upper=full),direction="both",trace=FALSE)
summary(selec1)
formula <- selec1$formula
selec2<-stepAIC(null,scope=list(upper=full),direction="forward",trace=FALSE)
summary(selec2)
selec2$formula
selec3<-stepAIC(full,scope=list(upper=null),direction="backward",trace=FALSE)
summary(selec3)
selec3$formula
dput(names(selec3))
unique(data_norm$style)
library(caret)
data_norm$style <- as.factor(data_norm$style)
levels(data_norm$style) <- c("No", "Yes")
control <- trainControl(method = "repeatedcv", number=4, repeats=5, savePredictions = "all", classProbs=TRUE)
data_norm$aux_style <- ifelse(data_norm$style == 1, "Yes", "No")
unique(data_norm$style)
data_norm$aux_style <- NULL
# Se transforma la varible dependiente a Yes y No
data_norm$style <- ifelse(
data_norm$style == 1,
"Yes",
"No"
)
unique(data_norm$style)
# Funcion para normalizar
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_norm <- as.data.frame(lapply(dataset[,-c(12, 13)], normalize))
data_norm$quality <- dataset$quality
data_norm$style <- dataset$style
library(dummies)
data_norm$quality_Cool <- dummy(data_norm$quality)[,1]
data_norm$quality_Poor <- dummy(data_norm$quality)[,2]
data_norm$quality <- NULL
library(readr)
dataset <- read_csv("wine_dataset.csv")
library(dplyr)
dataset <-dataset %>%
group_by(style) %>%
sample_frac(size = .05, replace = F)
dataset$quality <- ifelse(dataset$quality <= 5, "Poor", "Cool")
dataset$style <- ifelse(dataset$style=="red", 1, 0)
# Funcion para normalizar
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_norm <- as.data.frame(lapply(dataset[,-c(12, 13)], normalize))
data_norm$quality <- dataset$quality
data_norm$style <- dataset$style
library(dummies)
data_norm$quality_Cool <- dummy(data_norm$quality)[,1]
data_norm$quality_Poor <- dummy(data_norm$quality)[,2]
data_norm$quality <- NULL
# Se pasa la variable objetivo a factor.
data_norm$style <- as.factor(data_norm$style)
dput(names(data_norm))
c("fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar",
"chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide", "density",
"pH", "sulphates", "alcohol", "style", "quality_Cool", "quality_Poor"
)
full<-glm(style~.,data=data_norm, family = binomial(link="logit"))
null<-glm(style~1,data=data_norm, family = binomial(link="logit"))
library(MASS)
selec1<-stepAIC(null,scope=list(upper=full),direction="both",trace=FALSE)
summary(selec1)
formula <- selec1$formula
selec2<-stepAIC(null,scope=list(upper=full),direction="forward",trace=FALSE)
summary(selec2)
selec2$formula
selec3<-stepAIC(full,scope=list(upper=null),direction="backward",trace=FALSE)
summary(selec3)
selec3$formula
dput(names(selec3))
library(caret)
data_norm$style <- as.factor(data_norm$style)
levels(data_norm$style) <- c("No", "Yes")
control <- trainControl(method = "repeatedcv", number=4, repeats=5, savePredictions = "all", classProbs=TRUE)
data_norm$style == factor(1)
data_norm$style == as.factor(1)
as.numeric(data_norm$style) == 1
table(data_norm$style)
# Se transforma la varible dependiente a Yes y No
data_norm$style <- ifelse(
as.numeric(data_norm$style) == 1,
"Yes",
"No"
)
table(data_norm$style)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
dataset <- read_csv("wine_dataset.csv")
library(dplyr)
dataset <-dataset %>%
group_by(style) %>%
sample_frac(size = .05, replace = F)
dataset$quality <- ifelse(dataset$quality <= 5, "Poor", "Cool")
dataset$style <- ifelse(dataset$style=="red", 1, 0)
# Funcion para normalizar
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_norm <- as.data.frame(lapply(dataset[,-c(12, 13)], normalize))
data_norm$quality <- dataset$quality
data_norm$style <- dataset$style
library(dummies)
data_norm$quality_Cool <- dummy(data_norm$quality)[,1]
data_norm$quality_Poor <- dummy(data_norm$quality)[,2]
data_norm$quality <- NULL
# Se pasa la variable objetivo a factor.
data_norm$style <- as.factor(data_norm$style)
dput(names(data_norm))
c("fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar",
"chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide", "density",
"pH", "sulphates", "alcohol", "style", "quality_Cool", "quality_Poor"
)
full<-glm(style~.,data=data_norm, family = binomial(link="logit"))
null<-glm(style~1,data=data_norm, family = binomial(link="logit"))
library(MASS)
selec1<-stepAIC(null,scope=list(upper=full),direction="both",trace=FALSE)
summary(selec1)
formula <- selec1$formula
selec2<-stepAIC(null,scope=list(upper=full),direction="forward",trace=FALSE)
summary(selec2)
selec2$formula
selec3<-stepAIC(full,scope=list(upper=null),direction="backward",trace=FALSE)
summary(selec3)
selec3$formula
dput(names(selec3))
table(data_norm$style)
library(caret)
data_norm$style <- as.factor(data_norm$style)
levels(data_norm$style) <- c("No", "Yes")
control <- trainControl(method = "repeatedcv", number=4, repeats=5, savePredictions = "all", classProbs=TRUE)
table(data_norm$style)
source ("../UCM/Machine Learning/Practica ML/MachineLearning/librerias/cruzadas avnnet y log binaria.R")
# source ("../UCM/Machine Learning/Practica ML/MachineLearning/librerias/cruzada arbolbin.R")
source ("../UCM/Machine Learning/Practica ML/MachineLearning/librerias/cruzada rf binaria.R")
source ("../UCM/Machine Learning/Practica ML/MachineLearning/librerias/cruzada gbm binaria.R")
source ("../UCM/Machine Learning/Practica ML/MachineLearning/librerias/cruzada SVM binaria lineal.R")
source ("../UCM/Machine Learning/Practica ML/MachineLearning/librerias/cruzada SVM binaria polinomial.R")
source ("../UCM/Machine Learning/Practica ML/MachineLearning/librerias/cruzada SVM binaria RBF.R")
variables <- c("total_sulfur_dioxide", "density",  "residual_sugar",  "alcohol",
"volatile_acidity", "chlorides",  "free_sulfur_dioxide", "sulphates",
"citric_acid")
medias1<-cruzadalogistica(data=data_norm,
vardep="style",
listconti= variables,
listclass=c(""),
grupos=4,
sinicio=1234,
repe=5)
medias1bis<-as.data.frame(medias1[1])
medias1bis$modelo<-"Logistica"
predi1<-as.data.frame(medias1[2])
predi1$logi<-predi1$Yes
install.packages("styler")
styler:::style_active_file()
setwd("~/UCM/Productivizar un modelo")
styler:::style_active_file()
library(tidyverse)  # for tidy data analysis
library(farff)      # for reading arff file
library(missForest) # for imputing missing values
library(dummies)    # for creating dummy variables
library(caret)      # for modeling
install.packages("lubridate")
# Read File
data_file <- file.path("chronic_kidney_disease_full.arff")
data <- readARFF(data_file)
setwd("~/UCM/Productivizar un modelo/Codigo_R_datos_modelo_RData")
data <- readARFF(data_file)
# Impute NAs
data_imp <- missForest(data)
names(data)
data_file <- file.path("chronic_kidney_disease_full.arff")
data <- readARFF(data_file)
data$class
data[, -c("class")]
data[, ~c("class")]
class(data)
data[!names(data) %in% c("class")]
names(data)
# Impute NAs
# Warning: target variable must be excluded
data_imp <- missForest(data[!names(data) %in% c("class")])
data_imp$ximp
# Create dummies
data_imp_final <- cbind(data_imp$ximp, data$class)
set.seed(42)
index <- createDataPartition(data_imp_final$class, p = 0.9, list = FALSE)
train_data <- data_imp_final[index, ]
test_data  <- data_imp_final[-index, ]
data_imp_final$class
View(data_imp_final)
names(data_imp_final)[-1]
names(data_imp_final)[-1]
names(data_imp_final)[length(data_imp_final)]
names(data_imp_final)[length(data_imp_final)] <- "class"
set.seed(42)
index <- createDataPartition(data_imp_final$class, p = 0.9, list = FALSE)
train_data <- data_imp_final[index, ]
test_data  <- data_imp_final[-index, ]
model_rf <- caret::train(class ~ .,
data = train_data,
method = "rf", # random forest
trControl = trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = FALSE))
model_rf
train_data
# Reference:
# https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/
# Load libraries
library(tidyverse)  # for tidy data analysis
library(farff)      # for reading arff file
library(missForest) # for imputing missing values
library(dummies)    # for creating dummy variables
library(caret)      # for modeling
# File rar
# http://archive.ics.uci.edu/ml/machine-learning-databases/00336/Chronic_Kidney_Disease.rar
# Read File
data_file <- file.path("chronic_kidney_disease_full.arff")
data <- readARFF(data_file)
# Impute NAs
# Warning: target variable must be excluded
data_imp <- missForest(data[!names(data) %in% c("class")])
cbind(data_imp$ximp, data$class)
sum(is.na(data_imp))
table(data_file)
table(data_imp)
table(data_imp$ximp)
table(data$class)
sum(is.na(data$class))
table(data$class, exclude = FALSE)
sum(is.na(data))
install.packages("lime")
library(lime)       # for model explain
# Reference:
# https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/
# Load libraries
library(tidyverse)  # for tidy data analysis
library(farff)      # for reading arff file
library(missForest) # for imputing missing values
library(dummies)    # for creating dummy variables
library(caret)      # for modeling
library(lime)       # for model explain
# File rar
# http://archive.ics.uci.edu/ml/machine-learning-databases/00336/Chronic_Kidney_Disease.rar
# Read File
data_file <- file.path("chronic_kidney_disease_full.arff")
data <- readARFF(data_file)
# Impute NAs
# Warning: target variable must be excluded
data_imp <- missForest(data[!names(data) %in% c("class")])
# Create dummies
data_imp_final <- cbind(data_imp$ximp, data$class)
names(data_imp_final)[length(data_imp_final)] <- "class"
sum(is.na(datos))
sum(is.na(data))
data_imp_final
data_imp_final <- missForest(data_imp_final)
set.seed(42)
index <- createDataPartition(data_imp_final$class, p = 0.9, list = FALSE)
train_data <- data_imp_final[index, ]
test_data  <- data_imp_final[-index, ]
# modeling
model_rf <- caret::train(class ~ .,
data = train_data,
method = "rf", # random forest
trControl = trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = FALSE))
model_rf
data_imp_final$class
names(data_imp_final)[length(data_imp_final)] <- "class"
data_imp_final$class
# Reference:
# https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/
# Load libraries
library(tidyverse)  # for tidy data analysis
library(farff)      # for reading arff file
library(missForest) # for imputing missing values
library(dummies)    # for creating dummy variables
library(caret)      # for modeling
library(lime)       # for model explain
# File rar
# http://archive.ics.uci.edu/ml/machine-learning-databases/00336/Chronic_Kidney_Disease.rar
# Read File
data_file <- file.path("chronic_kidney_disease_full.arff")
data <- readARFF(data_file)
# Impute NAs
# Warning: target variable must be excluded
data_imp <- missForest(data[!names(data) %in% c("class")])
data_imp_final <- cbind(data_imp$ximp, data$class)
names(data_imp_final)[length(data_imp_final)]
names(data_imp_final)[length(data_imp_final)] <- "class"
table(data_imp_final$class)
data_imp_final_target_imp <- missForest(data_imp_final)$ximp
data_imp_final_target_imp
set.seed(42)
index <- createDataPartition(data_imp_final_target_imp$class, p = 0.9, list = FALSE)
train_data <- data_imp_final[index, ]
test_data  <- data_imp_final[-index, ]
model_rf <- caret::train(class ~ .,
data = train_data,
method = "rf", # random forest
trControl = trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = FALSE))
model_rf
train_data <- data_imp_final_target_imp[index, ]
test_data  <- data_imp_final_target_imp[-index, ]
model_rf <- caret::train(class ~ .,
data = train_data,
method = "rf", # random forest
trControl = trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = FALSE))
model_rf
save(model_rf,   file = "model_rf.RData")
save(train_data, file = "train_data.RData")
save(test_data,  file = "test_data.RData")
iinstall.packages("plumber")
install.packages("plumber")
names(data_imp_final_target_imp)
table(data_imp_final_target_imp$rbc)
table(data_imp_final_target_imp$pc)
table(data_imp_final_target_imp$pcc)
table(data_imp_final_target_imp$ba)
table(data_imp_final_target_imp$htn)
table(data_imp_final_target_imp$dm)
table(data_imp_final_target_imp$cad)
table(data_imp_final_target_imp$appet)
table(data_imp_final_target_imp$pee)
table(data_imp_final_target_imp$p)
table(data_imp_final_target_imp$pe)
load("train_data.RData")
sapply(train_data, clas)
sapply(train_data, class)
sapply(train_data[, is.factor(train_data)], class)
sapply(train_data[, class(train_data) == "factor"], class)
is.facet(train_data)
is.factor(train_data)
is.factor(train_data$bp)
is.factor(train_data$cad)
sapply(train_data, is.factor)
levels(train_data[sapply(train_data, is.factor)])
levels(train_data[, sapply(train_data, is.factor)])
train_data[, sapply(train_data, is.factor)]
sapply(train_data[, sapply(train_data, is.factor)], levels)
sapply(train_data[, sapply(train_data, is.factor)], levels)
sapply(train_data[, sapply(train_data, is.factor)], levels)[-1]
x <<- data.frame(0, 1, "normal", "abnormal", "present", "notpresent", "yes", "yes", "yes", "good", "yes", "yes",
stringsAsFactors = FALSE)
x
cat_levels       <<- sapply(train_data[, sapply(train_data, is.factor)], levels)[-1]
cat_levels
check.factor.levels <- function(expected, actual) {
length(setdiff(actual, expected)) == 0
}
x
check.factor.levels <- function(expected, actual) {
length(setdiff(actual, expected)) == 0
}
check.factor.levels(c("normal", "abnormal"), levels(x$X.normal.))
names(cat_levels)
cat_levels       <<- sapply(train_data[, sapply(test_data, is.factor)], levels)[-1]
cat_levels
names(input_data_int) <- names(cat_levels)
names(x) <- names(cat_levels)
sapply(train_data[, sapply(train_data, is.factor)], levels)
cat_levels       <<- sapply(train_data[, sapply(train_data, is.factor)], levels)[-1, -14]
names(x) <- names(cat_levels)
cat_levels       <<- sapply(train_data[, sapply(train_data, is.factor)], levels)[c(-1, -14)]
names(x) <- names(cat_levels)
x
lapply(cat_levels, function(x) print(names(x)))
lapply(cat_levels, function(x) print(x))
cat_levels
mapply(x, cat_levels)
mapply(c, x, cat_levels)
mapply(%in%, x, cat_levels)
mapply(function(x, y) x%in% y, x, cat_levels)
sum(mapply(function(x, y) x%in% y, x, cat_levels))
factor_in_levels <- mapply(function(x, y) x%in% y, x, cat_levels)
all(factor_in_levels)
!all(factor_in_levels)
source('~/UCM/Productivizar un modelo/Codigo_R_datos_modelo_RData/plumber.R')
getwd()
library(plumber)
r <- plumb("00_Ejemplo_01_.R")
r$run(port=8000)
library(plumber)
r <- plumb("plumber.R")
r$run(port=8000)
library(plumber)
r <- plumb("plumber.R")
r$run(port=8000)
library(plumber)
r <- plumb("plumber.R")
r$run(port=8000)
library(plumber)
r <- plumb("plumber.R")
r$run(port=8000)
# Reference:
# https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/
# Load libraries
library(tidyverse)  # for tidy data analysis
library(farff)      # for reading arff file
library(missForest) # for imputing missing values
library(dummies)    # for creating dummy variables
library(caret)      # for modeling
library(lime)       # for model explain
# File rar
# http://archive.ics.uci.edu/ml/machine-learning-databases/00336/Chronic_Kidney_Disease.rar
# Read File
data_file <- file.path("chronic_kidney_disease_full.arff")
data <- readARFF(data_file)
apply(data, class)
sapply(data, class)
data$sg
data$sg <- as.numeric(data$sg)
# Impute NAs
# Warning: target variable must be excluded
data_imp <- missForest(data[!names(data) %in% c("class")])
# Create dummies
data_imp_final <- cbind(data_imp$ximp, data$class)
names(data_imp_final)[length(data_imp_final)] <- "class"
# data_dummy <- dummy.data.frame(dplyr::select(data_imp_final, -class), sep = "_")
# data <- cbind(dplyr::select(data_imp_final, class), scale(data_dummy,
#                                                          center = apply(data_dummy, 2, min),
#                                                          scale = apply(data_dummy, 2, max)))
data_imp_final_target_imp <- missForest(data_imp_final)$ximp
# training and test set
set.seed(42)
index <- createDataPartition(data_imp_final_target_imp$class, p = 0.9, list = FALSE)
train_data <- data_imp_final_target_imp[index, ]
test_data  <- data_imp_final_target_imp[-index, ]
# modeling
model_rf <- caret::train(class ~ .,
data = train_data,
method = "rf", # random forest
trControl = trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = FALSE))
model_r
# Save files
save(model_rf,   file = "model_rf.RData")
save(train_data, file = "train_data.RData")
save(test_data,  file = "test_data.RData")
# # Train and Test for LIME
# train_x <- dplyr::select(train_data, -class)
# test_x <- dplyr::select(test_data, -class)
#
# train_y <- dplyr::select(train_data, class)
# test_y <- dplyr::select(test_data, class)
library(plumber)
r <- plumb("plumber.R")
r$run(port=8000)
library(plumber)
library(plumber)
r <- plumb("plumber.R")
r$run(port=8000)
library(plumber)
r <- plumb("plumber.R")
r$run(port=8000)
library(plumber)
r <- plumb("plumber.R")
r$run(port=8000)
library(plumber)
r <- plumb("plumber.R")
r$run(port=8000)
