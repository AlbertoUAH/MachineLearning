print(ggplot(union, aes(x = modelo, y = auc)) +
geom_boxplot(fill = fill, colour = line,
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo"))
return(union)
}
# Funcion para obtener la matriz de confusion de las predicciones resultantes
matriz_confusion_predicciones <- function(modelo = "glm", formula, dataset, corte) {
if (modelo == "glm") {
modelo <- glm(formula,
data = dataset,
family = binomial(link="logit")
)
}
pred <- predict(modelo, dataset, type = "prob")
pred_vector <- as.factor(ifelse(
pred$No > corte,
"No",
"Yes"
))
matriz_confusion <- confusionMatrix(dataset$target, pred_vector)
return(matriz_confusion)
}
# comparacion modelos redes neuronales
comparar_modelos_red <- function(dataset, target, lista.continua, sizes,
decays, grupos, repe, iteraciones) {
union  <- data.frame(tasa = numeric(), auc = numeric(), modelo = character())
for (i in seq(1:length(sizes))) {
cvnnet.candidato <- cruzadaavnnetbin(data=dataset,vardep=target,
listconti=lista.continua,
listclass=c(""),
grupos=grupos,sinicio=1234,
repe=repe, size=sizes[i],
decay=decays[i],repeticiones=repe,
itera=iteraciones)
modelo <- paste0("NODOS: ", sizes[i] , " - DECAY: ", decays[i])
medias.df <- data.frame(cvnnet.candidato[1])
medias.df$modelo <- paste0(modelo)
union <- rbind(union, medias.df)
print(paste0(modelo, ": completed!"))
}
fill <- "#4271AE"
line <- "#1F3552"
print(ggplot(union, aes(x = modelo, y = tasa)) +
geom_boxplot(fill = fill, colour = line,
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo"))
print(ggplot(union, aes(x = modelo, y = auc)) +
geom_boxplot(fill = fill, colour = line,
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo"))
return(union)
}
# Funcion para mostrar el err. rate en Bagging
mostrar_err_rate <- function(train.err.rate1, train.err.rate2) {
plot(train.err.rate1, col = 'red', type = 'l',
main = 'Error rate by nÂº trees', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
lines(train.err.rate2, col = 'blue')
#lines(test.err.rate1, col = 'green')
#lines(test.err.rate2, col = 'purple')
abline(h = 0.1)
legend("topright", legend = c("OOB: 5 variables (train)","OOB: 6 variables (train)") ,
col = c('red', 'blue') , bty = "n", horiz = FALSE,
lty=1, cex = 0.75)
}
# Funcion para el tuneo de un modelo bagging
tuneo_bagging <- function(dataset, target, lista.continua, nodesizes, sampsizes,
mtry, ntree, grupos, repe, replace = TRUE) {
lista.rf <- list()
for(x in apply(data.frame(expand.grid(nodesizes, sampsizes)),1,as.list)) {
salida <- cruzadarfbin(data=dataset, vardep=target,
listconti=lista.continua,
listclass=c(""),
grupos=grupos,sinicio=1234,repe=repe,nodesize=x$Var1,
mtry=mtry,ntree=ntree, sampsize=x$Var2, replace = replace)
cat(x$Var1, "+",  x$Var2 , "-> FINISHED\n")
salida$modelo <- paste0(x$Var1, "+",  x$Var2)
lista.rf <- c(lista.rf, list(salida))
}
union <- do.call(rbind.data.frame, lista.rf)
fill <- "#4271AE"
line <- "#1F3552"
print(colnames(union))
print(ggplot(union, aes(x = modelo, y = tasa)) +
geom_boxplot(fill = fill, colour = line,
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo"))
print(ggplot(union, aes(x = modelo, y = auc)) +
geom_boxplot(fill = fill, colour = line,
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo"))
return(union)
}
train_rf_model <- function(dataset, formula, mtry, ntree, grupos, repe,
nodesize, seed) {
set.seed(seed)
rfgrid <- expand.grid(mtry=mtry)
control <- trainControl(method = "repeatedcv",number=grupos, repeats=repe,
savePredictions = "all",classProbs = TRUE)
rf<- train(formula,data=dataset,
method="rf",trControl=control,tuneGrid=rfgrid,
linout = FALSE,ntree=ntree,nodesize=nodesize,replace=TRUE,
importance=TRUE)
return(rf)
}
surgical_test_data <- fread("./data/surgical_test_data.csv", data.table = FALSE)
names(surgical_test_data)[35] <- "target"
surgical_test_data$target     <- as.factor(surgical_test_data$target)
# Aplico caret y construyo modelos finales
control <- trainControl(method = "repeatedcv",number=5,repeats=5,
savePredictions = "all",classProbs=TRUE)
#-- Modelo 1
rfgrid.1 <-expand.grid(mtry=mtry.1)
set.seed(1234)
bagging_1 <- train(as.formula(paste0(target, "~" , paste0(var_modelo1, collapse = "+"))),
data=surgical_dataset, method="rf", trControl = control,tuneGrid = rfgrid.1,
nodesize = 30, sampsize = 2000, ntree = n.trees, replace = TRUE)
matriz_conf_1 <- matriz_confusion_predicciones(bagging_1, NULL, surgical_test_data, 0.5)
surgical_test_data <- fread("./data/surgical_test_data.csv", data.table = FALSE)
names(surgical_test_data)[35] <- "target"
surgical_test_data$target     <- as.factor(surgical_test_data$target)
# Aplico caret y construyo modelos finales
control <- trainControl(method = "repeatedcv",number=5,repeats=5,
savePredictions = "all",classProbs=TRUE)
#-- Modelo 1
rfgrid.1 <-expand.grid(mtry=mtry.1)
set.seed(1234)
bagging_1 <- train(as.formula(paste0(target, "~" , paste0(var_modelo1, collapse = "+"))),
data=surgical_dataset, method="rf", trControl = control,tuneGrid = rfgrid.1,
nodesize = 20, sampsize = 2000, ntree = n.trees, replace = TRUE)
matriz_conf_1 <- matriz_confusion_predicciones(bagging_1, NULL, surgical_test_data, 0.5)
rfgrid.2 <-expand.grid(mtry=mtry.2)
set.seed(1234)
bagging_2 <- train(as.formula(paste0(target, "~" , paste0(var_modelo2, collapse = "+"))),
data=surgical_dataset, method="rf", trControl = control,tuneGrid = rfgrid.2,
nodesize = 30, sampsize = 2000, ntree = n.trees, replace = TRUE)
matriz_conf_2 <- matriz_confusion_predicciones(bagging_2, NULL, surgical_test_data, 0.5)
rm(rfgrid.1)
rm(rfgrid.2)
matriz_conf_1
matriz_conf_2
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
matriz_conf_2$table
matriz_conf_1$table
union_bagging_modelo1
union_bagging_modelo2
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
matriz_conf_1
matriz_conf_2
union_bagging_modelo1
# ------------- Bagging ---------------
# Objetivo: elaborar el mejor modelo de bagging de acuerdo
#           a los valores de prediccion obtenidos tras variar los parametros
#           sampsize,
# Autor: Alberto Fernandez Hernandez
#--- Librerias
suppressPackageStartupMessages({
library(data.table)    # Lectura de ficheros mucho mas rapido que read.csv
library(parallel)      # Paralelizacion de funciones (I)
library(doParallel)    # Paralelizacion de funciones (II)
library(caret)         # Recursive Feature Elimination
library(randomForest)  # Seleccion del numero de arboles
library(readxl)        # Lectura de ficheros Excel
source("./librerias/librerias_propias.R")
})
#--- Creamos el cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
#--- Lectura dataset depurado
surgical_dataset <- fread("./data/surgical_dataset_final.csv", data.table = FALSE)
surgical_dataset$target <- as.factor(surgical_dataset$target)
# Separamos variable objetivo del resto
target <- "target"
#--- Variables de los modelos candidatos
#--  Modelo 1
var_modelo1 <- c("mortality_rsi", "ccsMort30Rate", "bmi", "month.8",
"Age", "baseline_osteoart")
#-- Modelo 2
var_modelo2 <- c("Age", "mortality_rsi", "ccsMort30Rate", "bmi", "ahrq_ccs")
# En primer lugar, tuneamos el numero de variables independientes del modelo
# En nuestro caso, 8 para el modelo 1 y 5 para el modelo 2
mtry.1 <- 8
mtry.2 <- 5
#--- Seleccion del numero de arboles
#--  Modelo 1
set.seed(1234)
rfbis.1<-randomForest(factor(target)~mortality_rsi+ccsMort30Rate+bmi+month.8+dow.0+Age+moonphase.0+baseline_osteoart,
data=surgical_dataset,
mtry=mtry.1,ntree=5000,nodesize=10,replace=TRUE)
#-- Modelo 2
set.seed(1234)
rfbis.2<-randomForest(factor(target)~Age+mortality_rsi+ccsMort30Rate+bmi+ahrq_ccs,
data=surgical_dataset,
mtry=mtry.2,ntree=5000,nodesize=10,replace=TRUE)
mostrar_err_rate(rfbis.2$err.rate[, 1], rfbis.1$err.rate[, 1])
# rfbis.2$test$err.rate[, 1], rfbis.1$test$err.rate[, 1])
#-- Ampliamos entre 0 y 2000 arboles
mostrar_err_rate(rfbis.2$err.rate[c(0:2000), 1], rfbis.1$err.rate[c(0:2000), 1])
#rfbis.2$test$err.rate[c(0:2000), 1], rfbis.1$test$err.rate[c(0:2000), 1])
#-- Ampliamos entre 0 y 1000 arboles
mostrar_err_rate(rfbis.2$err.rate[c(0:1000), 1], rfbis.1$err.rate[c(0:1000), 1])
#rfbis.2$test$err.rate[c(0:1000), 1], rfbis.1$test$err.rate[c(0:1000), 1])
n.trees <- 300
# Sampsize maximo: (k-1) * n => (4/5) * 5854 = 4683.2 ~ 4600 obs.
# Conclusion: sampsize maximo: 4600 obs. (de forma aproximada)
#--  Modelo 1
sampsizes.1 <- list(1, 100, 500, 1000, 2000, 3000, 4600)
nodesizes.1 <- list(5, 10, 20, 30, 40, 50, 60, 100, 150)
bagging_modelo1 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees, grupos = 5, repe = 5)
bagging_modelo1$nodesizes <- as.numeric(c(data.frame(strsplit(bagging_modelo1$modelo, '+', fixed = T))[1,]))
bagging_modelo1$sampsizes <- as.numeric(c(data.frame(strsplit(bagging_modelo1$modelo, '+', fixed = T))[2,]))
#-- Distribucion de la tasa de error
ggplot(bagging_modelo1, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=3, shape = 18) +
ggtitle("Distribucion de la tasa de error por sampsizes y nodesizes (Modelo 1)")
ggsave('./charts/bagging/03_distribucion_tasa_error_modelo1.png')
#-- Distribucion de la tasa de error
ggplot(bagging_modelo1, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion de la tasa de error por sampsizes y nodesizes (Modelo 1)")
ggsave('./charts/bagging/03_distribucion_tasa_error_modelo1.png')
ggplot(bagging_modelo1, aes(x=factor(sampsizes), y=auc,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 1)")
ggsave('./charts/bagging/distribuciones/03_distribucion_auc_modelo1.png')
nodesizes.1 <- list(20)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4600)
bagging_modelo1_2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees, grupos = 5, repe = 5)
nodesizes.1 <- list(30)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4600)
bagging_modelo1_3 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees, grupos = 5, repe = 5)
var_modelo1
mtry.1
suppressPackageStartupMessages({
library(data.table)    # Lectura de ficheros mucho mas rapido que read.csv
library(parallel)      # Paralelizacion de funciones (I)
library(doParallel)    # Paralelizacion de funciones (II)
library(caret)         # Recursive Feature Elimination
library(randomForest)  # Seleccion del numero de arboles
library(readxl)        # Lectura de ficheros Excel
source("./librerias/librerias_propias.R")
})
#--- Creamos el cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
#--- Lectura dataset depurado
surgical_dataset <- fread("./data/surgical_dataset_final.csv", data.table = FALSE)
surgical_dataset$target <- as.factor(surgical_dataset$target)
# Separamos variable objetivo del resto
target <- "target"
#--- Variables de los modelos candidatos
#--  Modelo 1
var_modelo1 <- c("mortality_rsi", "ccsMort30Rate", "bmi", "month.8",
"Age", "baseline_osteoart")
#-- Modelo 2
var_modelo2 <- c("Age", "mortality_rsi", "ccsMort30Rate", "bmi", "ahrq_ccs")
# En primer lugar, tuneamos el numero de variables independientes del modelo
# En nuestro caso, 8 para el modelo 1 y 5 para el modelo 2
mtry.1 <- 6
mtry.2 <- 5
#--- Seleccion del numero de arboles
#--  Modelo 1
set.seed(1234)
rfbis.1<-randomForest(factor(target)~mortality_rsi+ccsMort30Rate+bmi+month.8+dow.0+Age+moonphase.0+baseline_osteoart,
data=surgical_dataset,
mtry=mtry.1,ntree=5000,nodesize=10,replace=TRUE)
#-- Modelo 2
set.seed(1234)
rfbis.2<-randomForest(factor(target)~Age+mortality_rsi+ccsMort30Rate+bmi+ahrq_ccs,
data=surgical_dataset,
mtry=mtry.2,ntree=5000,nodesize=10,replace=TRUE)
mostrar_err_rate(rfbis.2$err.rate[, 1], rfbis.1$err.rate[, 1])
# rfbis.2$test$err.rate[, 1], rfbis.1$test$err.rate[, 1])
#-- Ampliamos entre 0 y 2000 arboles
mostrar_err_rate(rfbis.2$err.rate[c(0:2000), 1], rfbis.1$err.rate[c(0:2000), 1])
#rfbis.2$test$err.rate[c(0:2000), 1], rfbis.1$test$err.rate[c(0:2000), 1])
#-- Ampliamos entre 0 y 1000 arboles
mostrar_err_rate(rfbis.2$err.rate[c(0:1000), 1], rfbis.1$err.rate[c(0:1000), 1])
#rfbis.2$test$err.rate[c(0:1000), 1], rfbis.1$test$err.rate[c(0:1000), 1])
#-- Conclusion: con 300 arboles parece ser suficiente
#--- Tuneo de modelos
n.trees <- 300
# Sampsize maximo: (k-1) * n => (4/5) * 5854 = 4683.2 ~ 4600 obs.
# Conclusion: sampsize maximo: 4600 obs. (de forma aproximada)
#--  Modelo 1
sampsizes.1 <- list(1, 100, 500, 1000, 2000, 3000, 4600)
nodesizes.1 <- list(5, 10, 20, 30, 40, 50, 60, 100, 150)
bagging_modelo1 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees, grupos = 5, repe = 5)
bagging_modelo1$nodesizes <- as.numeric(c(data.frame(strsplit(bagging_modelo1$modelo, '+', fixed = T))[1,]))
bagging_modelo1$sampsizes <- as.numeric(c(data.frame(strsplit(bagging_modelo1$modelo, '+', fixed = T))[2,]))
mostrar_err_rate(rfbis.2$err.rate[, 1], rfbis.1$err.rate[, 1])
mostrar_err_rate(rfbis.2$err.rate[c(0:2000), 1], rfbis.1$err.rate[c(0:2000), 1])
mostrar_err_rate(rfbis.2$err.rate[c(0:1000), 1], rfbis.1$err.rate[c(0:1000), 1])
set.seed(1234)
rfbis.1<-randomForest(factor(target)~mortality_rsi+ccsMort30Rate+bmi+month.8+Age+baseline_osteoart,
data=surgical_dataset,
mtry=mtry.1,ntree=5000,nodesize=10,replace=TRUE)
#-- Modelo 2
set.seed(1234)
rfbis.2<-randomForest(factor(target)~Age+mortality_rsi+ccsMort30Rate+bmi+ahrq_ccs,
data=surgical_dataset,
mtry=mtry.2,ntree=5000,nodesize=10,replace=TRUE)
mostrar_err_rate(rfbis.2$err.rate[, 1], rfbis.1$err.rate[, 1])
mostrar_err_rate(rfbis.2$err.rate[c(0:2000), 1], rfbis.1$err.rate[c(0:2000), 1])
mostrar_err_rate(rfbis.2$err.rate[c(0:1000), 1], rfbis.1$err.rate[c(0:1000), 1])
rm(n.trees)
n.trees.1 <- 500
n.trees.2 <- 800
sampsizes.1 <- list(1, 100, 500, 1000, 2000, 3000, 4600)
nodesizes.1 <- list(5, 10, 20, 30, 40, 50, 60, 100, 150)
bagging_modelo1 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 5)
bagging_modelo1$nodesizes <- as.numeric(c(data.frame(strsplit(bagging_modelo1$modelo, '+', fixed = T))[1,]))
bagging_modelo1$sampsizes <- as.numeric(c(data.frame(strsplit(bagging_modelo1$modelo, '+', fixed = T))[2,]))
var_modelo1
sampsizes.1 <- list(1, 100, 500, 1000, 2000, 3000, 4600)
nodesizes.1 <- list(5, 10, 20, 30, 40, 50, 60, 100, 150)
bagging_modelo1 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 5)
bagging_modelo1$nodesizes <- as.numeric(c(data.frame(strsplit(bagging_modelo1$modelo, '+', fixed = T))[1,]))
bagging_modelo1$sampsizes <- as.numeric(c(data.frame(strsplit(bagging_modelo1$modelo, '+', fixed = T))[2,]))
ggplot(bagging_modelo1, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion de la tasa de error por sampsizes y nodesizes (Modelo 1)")
ggsave('./charts/bagging/distribuciones/03_distribucion_tasa_error_modelo1.png')
ggplot(bagging_modelo1, aes(x=factor(sampsizes), y=auc,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 1)")
ggsave('./charts/bagging/distribuciones/03_distribucion_auc_modelo1.png')
n.trees.1
mostrar_err_rate(rfbis.2$err.rate[c(0:2000), 1], rfbis.1$err.rate[c(0:2000), 1])
var_modelo1
nodesizes.1 <- list(20)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4600)
bagging_modelo1_2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 5)
nodesizes.1 <- list(30)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4600)
bagging_modelo1_3 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 5)
nodesizes.1 <- list(40)
sampsizes.1 <- list(1500, 2000)
bagging_modelo1_4 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 5)
nodesizes.1 <- list(40)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4600)
bagging_modelo1_4 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 5)
nodesizes.1 <- list(50)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4600)
bagging_modelo1_5 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 5)
# No merece demasiado la pena, dado que el AUC apenas supera el 0.910 en el mejor de los casos
# y la tasa de error el 0.10
rm(bagging_modelo1_5)
union_bagging_modelo1 <- rbind(
bagging_modelo1_2[bagging_modelo1_2$modelo == "20+1500", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "30+1000", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "30+1500", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "30+2000", ],
bagging_modelo1_4[bagging_modelo1_4$modelo == "40+1500", ],
bagging_modelo1_4[bagging_modelo1_4$modelo == "40+2000", ]
)
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,tasa, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = tasa)) +
geom_boxplot(fill = "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggsave("./charts/bagging/distribuciones/03_comparacion_final_tasa_modelo1_5rep.png")
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,auc, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = auc)) +
geom_boxplot(fill = "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
ggsave("./charts/bagging/distribuciones/03_comparacion_final_auc_modelo1_5rep.png")
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
nodesizes.1 <- list(20)
sampsizes.1 <- list(1500)
bagging_modelo1_2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
# 1000-1500-2000 ofrecen buen resultado AUC (1500 y 2500 alta varianza en AUC pero bajo sesgo)
# En cambio, con un sampsize de 1000 presenta una tasa de fallos ligeramente superior
nodesizes.1 <- list(30)
sampsizes.1 <- list(1000, 1500, 2000)
bagging_modelo1_3 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
# Con 1500-2000 filas se obtiene un AUC alto y con poca varianza y menor sesgo
nodesizes.1 <- list(40)
sampsizes.1 <- list(1500, 2000)
bagging_modelo1_4 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
union_bagging_modelo1 <- rbind(
bagging_modelo1_2[bagging_modelo1_2$modelo == "20+1500", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "30+1000", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "30+1500", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "30+2000", ],
bagging_modelo1_4[bagging_modelo1_4$modelo == "40+1500", ],
bagging_modelo1_4[bagging_modelo1_4$modelo == "40+2000", ]
)
#-- Distribucion de la tasa de error
#   Modelos candidatos: 20+2000 - 20+1500 - 40+2000 (orden descendente)
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,tasa, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = tasa)) +
geom_boxplot(fill = "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggsave("./charts/bagging/distribuciones/03_comparacion_final_tasa_modelo1_10rep.png")
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,auc, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = auc)) +
geom_boxplot(fill = "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
ggsave("./charts/bagging/distribuciones/03_comparacion_final_auc_modelo1_10rep.png")
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
union_bagging_modelo1
var_modelo2
n.trees.2
mtry.2
mostrar_err_rate(rfbis.2$err.rate[c(0:2000), 1], rfbis.1$err.rate[c(0:2000), 1])
sampsizes.2 <- list(1, 100, 500, 1000, 2000, 3000, 4600)
nodesizes.2 <- list(5, 10, 20, 30, 40, 50, 60, 100, 150)
bagging_modelo2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 5)
bagging_modelo2$nodesizes <- as.numeric(c(data.frame(strsplit(bagging_modelo2$modelo, '+', fixed = T))[1,]))
bagging_modelo2$sampsizes <- as.numeric(c(data.frame(strsplit(bagging_modelo2$modelo, '+', fixed = T))[2,]))
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=3, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
ggsave("./charts/bagging/distribuciones/03_distribucion_tasa_error_modelo2.png")
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=auc,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=3, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=auc,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
g
ggsave("./charts/bagging/distribuciones/03_distribucion_auc_modelo2.png")
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
