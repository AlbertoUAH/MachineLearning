rf_modelo1_aux <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 10,
sampsizes = 1, mtry = mtry.1[i],
ntree = n.trees.1[i], grupos = 5, repe = 5)
rf_modelo1_aux$modelo <- rep(paste0(mtry.1[i],"-",n.trees.1), 5)
rf_modelo1 <- rbind(rf_modelo1, rf_modelo1_aux)
}
rm(rf_modelo1_aux)
plot(err.rates.1[,1], col = rgb(1,0,0, alpha = 0.4), type = 'l',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
lines(err.rates.1[,2], col = 'darkorange2')
lines(err.rates.1[,3], col = 'blue')
lines(err.rates.1[,4], col = 'darkgreen')
legend("topright", legend = c("OOB: MODELO 1 - MTRY 5", "OOB: MODELO 1 - MTRY 2", "OOB: MODELO 1 - MTRY 3", "OOB: MODELO 1 - MTRY 4") ,
col = c('red', 'orange', 'blue', 'darkgreen') , bty = "n", horiz = FALSE,
lty=1, cex = 0.75)
mtry.1 <- c(2,3,4,5)
n.trees.1 <- c(2500, 2000, 1500, 800)
#   Hacemos prueba con mtrys
rf_modelo1 <- data.frame()
for(i in seq(1, 4)) {
rf_modelo1_aux <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 10,
sampsizes = 1, mtry = mtry.1[i],
ntree = n.trees.1[i], grupos = 5, repe = 5)
rf_modelo1_aux$modelo <- rep(paste0(mtry.1[i],"-",n.trees.1), 5)
rf_modelo1 <- rbind(rf_modelo1, rf_modelo1_aux)
}
rm(rf_modelo1_aux)
mtry.1 <- c(2,3,4,5)
n.trees.1 <- c(2500, 2000, 1500, 800)
#   Hacemos prueba con mtrys
rf_modelo1 <- data.frame()
for(i in seq(1, 4)) {
rf_modelo1_aux <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 10,
sampsizes = 1, mtry = mtry.1[i],
ntree = n.trees.1[i], grupos = 5, repe = 5)
rf_modelo1_aux$modelo <- rep(paste0(mtry.1[i],"-",n.trees.1[i]), 5)
rf_modelo1 <- rbind(rf_modelo1, rf_modelo1_aux)
}
rm(rf_modelo1_aux)
rf_modelo1
ggplot(rf_modelo1, aes(x = modelo, y = tasa)) +
geom_boxplot(fill = "#4271AE", line = "#1F3552", alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggplot(rf_modelo1, aes(x = modelo, y = auc)) +
geom_boxplot(fill = "#4271AE", line = "#1F3552", alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
plot(err.rates.1[,1], col = rgb(1,0,0, alpha = 0.4), type = 'p',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
points(err.rates.1[,2], col = 'darkorange2')
points(err.rates.1[,3], col = 'blue')
points(err.rates.1[,4], col = 'darkgreen')
legend("topright", legend = c("OOB: MODELO 1 - MTRY 5", "OOB: MODELO 1 - MTRY 2", "OOB: MODELO 1 - MTRY 3", "OOB: MODELO 1 - MTRY 4") ,
col = c('red', 'orange', 'blue', 'darkgreen') , bty = "n", horiz = FALSE,
lty=1, cex = 0.75)
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/RandomForest.RData")
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/SeleccionVariables.RData")
candidato.bic.3 <- c("mortality_rsi", "ccsMort30Rate", "bmi", "month.8", "Age")
candidato.bic.4 <- c("mortality_rsi", "bmi", "month.8", "Age")
candidato.aic.3 <- c("mortality_rsi", "ahrq_ccs", "bmi", "month.8", "Age")
candidato.rfe.2 <- c("Age", "mortality_rsi", "bmi", "ahrq_ccs")
candidatos_4         <- list(candidato.aic, candidato.aic.2, candidato.aic.3, candidato.bic, candidato.bic.2, candidato.bic.3,
candidato.bic.4,candidato.rfe.lr, candidato.rfe.lr.2, candidato.rfe.rf, candidato.rfe.2)
nombres_candidatos_4 <- c("LOGISTICA AIC", "LOGISTICA AIC (sin 2 variables)" , "LOGISTICA AIC (TOP 5)" ,"LOGISTICA BIC", "LOGISTICA BIC (sin asa.status)" ,
"LOGISTICA BIC (TOP 5)", "LOGISTICA BIC (TOP 4)", "RFE LR TOP 18", "RFE LR TOP 3", "RFE RF TOP 5", "RFE RF TOP 4")
union4 <- cruzada_logistica(surgical_dataset, target, candidatos_4, nombres_candidatos_4,
grupos = 5, repe = 5)
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/RandomForest.RData")
plot(err.rates.2[,1], col = 'red', type = 'l',
main = 'Error rate by nº trees (Modelo 2)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
lines(err.rates.1[,2], col = 'darkorange2')
lines(err.rates.2[,3], col = 'blue')
lines(err.rates.2[,4], col = 'darkgreen')
legend("topright", legend = c("OOB: MODELO 2 - MTRY 5", "OOB: MODELO 2 - MTRY 2", "OOB: MODELO 2 - MTRY 3", "OOB: MODELO 2 - MTRY 4") ,
col = c('red', 'blue', 'darkgreen') , bty = "n", horiz = FALSE,
lty=1, cex = 0.75)
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/RedesNeuronales.RData")
modelos_actuales
modelos_actuales
modelos_actuales_zoomed <- modelos_actuales[modelos_actuales$modelo %in% c("AVNNET MODELO 1", "AVNNET MODELO 2"), ]
modelos_actuales_zoomed
modelos_actuales_zoomed$modelo <- with(modelos_actuales_zoomed,
reorder(modelo,tasa, mean))
ggplot(modelos_actuales_zoomed, aes(x = modelo, y = tasa)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo (solo AVNNET)")
modelos_actuales_zoomed$modelo <- with(modelos_actuales_zoomed,
reorder(modelo,auc, mean))
ggplot(modelos_actuales_zoomed, aes(x = modelo, y = auc)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (solo AVNNET)")
modelos_actuales_zoomed <- modelos_actuales[modelos_actuales$modelo %in% c("AVNNET MODELO 1", "AVNNET MODELO 2"), ]
modelos_actuales_zoomed$modelo <- with(modelos_actuales_zoomed,
reorder(modelo,tasa, mean))
ggplot(modelos_actuales_zoomed, aes(x = modelo, y = tasa)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo (solo AVNNET)")
ggsave('./charts/avnnet/02_FINAL_tasa.jpeg')
modelos_actuales_zoomed$modelo <- with(modelos_actuales_zoomed,
reorder(modelo,auc, mean))
ggplot(modelos_actuales_zoomed, aes(x = modelo, y = auc)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (solo AVNNET)")
ggsave('./charts/comparativas/02_FINAL_auc.jpeg')
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/RedesNeuronales.RData")
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
modelos_actuales
modelos_actuales$modelo <- with(modelos_actuales,
reorder(modelo,auc, mean))
ggplot(modelos_actuales, aes(x = modelo, y = auc)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
modelos_actuales
modelos_actuales_zoomed <- modelos_actuales[modelos_actuales$modelo %in% c("BAG. MODELO 1", "BAG. MODELO 2", "BAG. MODELO 2 (no reemp)"), ]
modelos_actuales_zoomed$modelo <- with(modelos_actuales_zoomed,
reorder(modelo,tasa, mean))
ggplot(modelos_actuales_zoomed, aes(x = modelo, y = tasa)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo (solo BAGGING)")
ggsave('./charts/bagging/03_FINAL_tasa.jpeg')
modelos_actuales_zoomed$modelo <- with(modelos_actuales_zoomed,
reorder(modelo,auc, mean))
ggplot(modelos_actuales_zoomed, aes(x = modelo, y = auc)) +
geom_boxplot(fill =  "#4271AE", colour = "#1F3552",
alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo (solo BAGGING)")
ggsave('./charts/bagging/03_FINAL_auc.jpeg')
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/RandomForest.RData")
err.rates.1 <- review_ntrees(surgical_dataset, factor(target)~mortality_rsi+ccsMort30Rate+bmi+month.8+Age,
mtry = c(2), ntree = 3000, nodesize = 10, seed = 1234)
plot(err.rates.1[,1], col = "red", type = 'l',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
plot(err.rates.1[,1], col = "red", type = 'p',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
err.rates.1 <- review_ntrees(surgical_dataset, factor(target)~mortality_rsi+ccsMort30Rate+bmi+month.8+Age,
mtry = c(2), ntree = 5000, nodesize = 10, seed = 1234)
plot(err.rates.1[,1], col = "red", type = 'p',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
plot(err.rates.1[,1], col = "red", type = 'l',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
err.rates.1[800,1]
err.rates.1[2500,1]
err.rates.1[3500,1]
err.rates.1[4500,1]
err.rates.1[5000,1]
err.rates.1[1000,1]
err.rates.1[500,1]
err.rates.1
plot(err.rates.1[,1], col = "red", type = 'l',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
err.rates.1[, 1]
err.rates.1[c(4000:5000), 1]
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
mostrar_err_rate(rfbis.2$err.rate[, 1], rfbis.1$err.rate[, 1])
rfbis.2$err.rate
rfbis.2$err.rate[c(500:800),1]
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/RandomForest.RData")
plot(err.rates.1[,1], col = "red", type = 'l',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
plot(err.rates.1[c(1:1000),1], col = "red", type = 'l',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
plot(err.rates.1[c(1:3000),1], col = "red", type = 'l',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
plot(err.rates.1[c(1:1000),1], col = "red", type = 'l',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
err.rates.1[c(3:5000),1]
err.rates.1[c(300:5000),1]
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
mostrar_err_rate(rfbis.1$err.rate[, 1], rfbis.1$err.rate[, 1])
mostrar_err_rate(rfbis.2$err.rate[, 1], rfbis.1$err.rate[, 1])
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/RandomForest.RData")
plot(err.rates.1[c(1:1000),1], col = "red", type = 'l',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
plot(err.rates.1[,1], col = "red", type = 'p',
main = 'Error rate by nº trees (Modelo 1)', xlab = 'Number of trees', ylab = 'Error rate', ylim = c(0.09, 0.13))
# ------------- Bagging ---------------
# Objetivo: elaborar el mejor modelo de Random Forest de acuerdo
#           a los valores de prediccion obtenidos tras variar los parametros
#           sampsize,
# Autor: Alberto Fernandez Hernandez
#--- Librerias
suppressPackageStartupMessages({
library(data.table)    # Lectura de ficheros mucho mas rapido que read.csv
library(parallel)      # Paralelizacion de funciones (I)
library(doParallel)    # Paralelizacion de funciones (II)
library(caret)         # Recursive Feature Elimination
library(randomForest)  # Seleccion del numero de arboles
library(readxl)        # Lectura de ficheros Excel
source("./librerias/librerias_propias.R")
})
#--- Creamos el cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
mtry.1 <- c(2,3,4,5)
n.trees.1 <- 800
#   Hacemos prueba con mtrys
rf_modelo1 <- data.frame()
rf_modelo1_10rep <- data.frame()
for(i in seq(1, 4)) {
rf_modelo1_aux <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 10,
sampsizes = 1, mtry = mtry.1[i],
ntree = n.trees.1, grupos = 5, repe = 5)
rf_modelo1_aux$modelo <- rep(paste0(mtry.1[i],"-",n.trees.1), 5)
rf_modelo1 <- rbind(rf_modelo1, rf_modelo1_aux)
rf_modelo1_5rep
}
rm(rf_modelo1_aux)
rm(rf_modelo1_10rep)
mtry.1 <- c(2,3,4,5)
n.trees.1 <- 800
#   Hacemos prueba con mtrys
rf_modelo1 <- data.frame()
for(i in seq(1, 4)) {
rf_modelo1_aux <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 10,
sampsizes = 1, mtry = mtry.1[i],
ntree = n.trees.1, grupos = 5, repe = 5)
rf_modelo1_aux$modelo <- rep(paste0(mtry.1[i],"-",n.trees.1), 5)
rf_modelo1 <- rbind(rf_modelo1, rf_modelo1_aux)
}
rm(rf_modelo1_aux)
ggplot(rf_modelo1, aes(x = modelo, y = tasa)) +
geom_boxplot(fill = "#4271AE", line = "#1F3552", alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggplot(rf_modelo1, aes(x = modelo, y = auc)) +
geom_boxplot(fill = "#4271AE", line = "#1F3552", alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
rm(rf_modelo1)
mtry.1 <- c(2,3,4,5)
n.trees.1 <- 800
#   Hacemos prueba con mtrys
rf_modelo1_5  <- data.frame()
rf_modelo1_10 <- data.frame()
for(i in seq(1, 4)) {
rf_modelo1_aux <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 10,
sampsizes = 1, mtry = mtry.1[i],
ntree = n.trees.1, grupos = 5, repe = 5)
rf_modelo1_aux$modelo <- rep(paste0(mtry.1[i],"-",n.trees.1), 5)
rf_modelo1_5 <- rbind(rf_modelo1_5, rf_modelo1_aux)
rf_modelo1_aux <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = 10,
sampsizes = 1, mtry = mtry.1[i],
ntree = n.trees.1, grupos = 5, repe = 10)
rf_modelo1_aux$modelo <- rep(paste0(mtry.1[i],"-",n.trees.1), 10)
rf_modelo1_10 <- rbind(rf_modelo1_10, rf_modelo1_aux)
}
rm(rf_modelo1_aux)
ggplot(rf_modelo1, aes(x = modelo, y = tasa)) +
geom_boxplot(fill = "#4271AE", line = "#1F3552", alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggplot(rf_modelo1_5, aes(x = modelo, y = tasa)) +
geom_boxplot(fill = "#4271AE", line = "#1F3552", alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggplot(rf_modelo1_5, aes(x = modelo, y = auc)) +
geom_boxplot(fill = "#4271AE", line = "#1F3552", alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
nodesizes.1 <- list(10)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500)
bagging_modelo1_2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
# ------------- Bagging ---------------
# Objetivo: elaborar el mejor modelo de bagging de acuerdo
#           a los valores de prediccion obtenidos tras variar los parametros
#           sampsize,
# Autor: Alberto Fernandez Hernandez
#--- Librerias
suppressPackageStartupMessages({
library(data.table)    # Lectura de ficheros mucho mas rapido que read.csv
library(parallel)      # Paralelizacion de funciones (I)
library(doParallel)    # Paralelizacion de funciones (II)
library(caret)         # Recursive Feature Elimination
library(randomForest)  # Seleccion del numero de arboles
library(readxl)        # Lectura de ficheros Excel
source("./librerias/librerias_propias.R")
})
#--- Creamos el cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
nodesizes.1 <- list(10)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500)
bagging_modelo1_2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
nodesizes.1 <- list(20)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500)
bagging_modelo1_3 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
nodesizes.1 <- list(30)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500)
bagging_modelo1_4 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
bagging_modelo1_4
nodesizes.1 <- list(50)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500)
bagging_modelo1_4 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
nodesizes.1 <- list(5)
sampsizes.1 <- list(1, 1000, 1500, 2000, 2500, 3000)
bagging_modelo1_5 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
rm(bagging_modelo1_5)
nodesizes.1 <- list(5)
sampsizes.1 <- list(1, 500, 1000, 1500, 2000, 2500)
bagging_modelo1_5 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1, mtry = mtry.1,
ntree = n.trees.1, grupos = 5, repe = 10)
rm(bagging_modelo1_5)
union_bagging_modelo1 <- rbind(
bagging_modelo1_2[bagging_modelo1_2$modelo == "10+1", ],
bagging_modelo1_2[bagging_modelo1_2$modelo == "10+500", ],
bagging_modelo1_2[bagging_modelo1_2$modelo == "10+1000", ],
bagging_modelo1_2[bagging_modelo1_2$modelo == "10+2000", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "20+1", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "20+500", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "20+1000", ],
bagging_modelo1_3[bagging_modelo1_3$modelo == "20+2000", ]
)
union_bagging_modelo1$config <- rep("5_folds-10_rep", 80)
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,tasa, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = tasa, col = config)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggsave("./charts/bagging/bis_03_comparacion_final_tasa_modelo1_5rep.png")
union_bagging_modelo1$modelo <- with(union_bagging_modelo1,
reorder(modelo,auc, mean))
ggplot(union_bagging_modelo1, aes(x = modelo, y = auc, col = config)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
nodesizes.1 <- list(10)
sampsizes.1_aux <- list(1, 500, 1000, 2000)
bagging_modelo1_2_10folds <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.1_aux, mtry = mtry.1,
ntree = n.trees.1, grupos = 10, repe = 20)
rm(sampsizes.1_aux)
# Nodesize 20
nodesizes.1 <- list(20)
sampsizes.2_aux <- list(1, 500, 1000, 2000)
bagging_modelo1_3_10folds <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.2_aux, mtry = mtry.1,
ntree = n.trees.1, grupos = 10, repe = 20)
rm(sampsizes.2_aux)
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
union_bagging_modelo1_10folds <- rbind(
bagging_modelo1_2_10folds[bagging_modelo1_2_10folds$modelo == "10+1", ],
bagging_modelo1_2_10folds[bagging_modelo1_2_10folds$modelo == "10+500", ],
bagging_modelo1_2_10folds[bagging_modelo1_2_10folds$modelo == "10+1000", ],
bagging_modelo1_2_10folds[bagging_modelo1_2_10folds$modelo == "10+2000", ],
bagging_modelo1_3_10folds[bagging_modelo1_3_10folds$modelo == "20+1", ],
bagging_modelo1_3_10folds[bagging_modelo1_3_10folds$modelo == "20+500", ],
bagging_modelo1_3_10folds[bagging_modelo1_3_10folds$modelo == "20+1000", ],
bagging_modelo1_3_10folds[bagging_modelo1_3_10folds$modelo == "20+2000", ]
)
union_bagging_modelo1_10folds$config <- rep("10_folds-20_rep", 80)
union_bagging_modelo1_final          <- rbind(union_bagging_modelo1,
union_bagging_modelo1_10folds)
union_bagging_modelo1_final$modelo <- with(union_bagging_modelo1_final,
reorder(modelo,tasa, mean))
ggplot(union_bagging_modelo1_final, aes(x = modelo, y = tasa, col = config)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("Tasa de fallos por modelo")
ggsave("./charts/bagging/bis_03_comparacion_final_tasa_modelo1_5_10_folds.png")
union_bagging_modelo1_final$modelo <- with(union_bagging_modelo1_final,
reorder(modelo,auc, mean))
ggplot(union_bagging_modelo1_final, aes(x = modelo, y = auc, col = config)) +
geom_boxplot(alpha = 0.7) +
scale_x_discrete(name = "Modelo") +
ggtitle("AUC por modelo")
ggsave("./charts/bagging/bis_03_comparacion_final_auc_modelo1_5_10_folds.png")
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
sampsizes.2 <- list(1, 100, 500, 1000, 2000, 3000, 4600)
nodesizes.2 <- list(5, 10, 20, 30, 40, 50, 100)
bagging_modelo2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 5)
bagging_modelo2$nodesizes <- as.numeric(c(data.frame(strsplit(bagging_modelo2$modelo, '+', fixed = T))[1,]))
bagging_modelo2$sampsizes <- as.numeric(c(data.frame(strsplit(bagging_modelo2$modelo, '+', fixed = T))[2,]))
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
bagging_modelo2$nodesizes <- as.numeric(c(data.frame(strsplit(bagging_modelo2$modelo, '+', fixed = T))[1,]))
bagging_modelo2$sampsizes <- as.numeric(c(data.frame(strsplit(bagging_modelo2$modelo, '+', fixed = T))[2,]))
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion de la tasa por sampsizes y nodesizes (Modelo 2)")
ggsave("./charts/bagging/distribuciones/03_distribucion_tasa_error_modelo2.png")
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=auc,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
ggplot(bagging_modelo2, aes(x=factor(sampsizes), y=auc,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 2)")
ggsave("./charts/bagging/distribuciones/03_distribucion_auc_modelo2.png")
nodesizes.2 <- list(10)
sampsizes.2 <- list(1, 500, 1000, 1500, 2000, 2500)
bagging_modelo2_2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 10)
ggplot(bagging_modelo1, aes(x=factor(sampsizes), y=auc,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion del AUC por sampsizes y nodesizes (Modelo 1)")
ggsave('./charts/bagging/distribuciones/03_distribucion_auc_modelo1.png')
ggplot(bagging_modelo1, aes(x=factor(sampsizes), y=tasa,
colour=factor(nodesizes))) +
geom_point(position=position_dodge(width=0.3),size=2, shape = 18) +
ggtitle("Distribucion de la tasa de error por sampsizes y nodesizes (Modelo 1)")
ggsave('./charts/bagging/distribuciones/03_distribucion_tasa_error_modelo1.png')
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/SeleccionVariables.RData")
show_vars_importance(rf_modelo_bic, "Importancia variables Random Forest (modelo BIC)")
show_vars_importance(rf_modelo_aic_2, "Importancia variables Random Forest (modelo AIC)")
show_vars_importance(rf_modelo_rfe, "Importancia variables Random Forest (modelo RFE RF TOP 5)")
candidato.bic.3 <- c("mortality_rsi", "ccsMort30Rate", "bmi", "month.8", "Age")
candidato.bic.4 <- c("mortality_rsi", "bmi", "month.8", "Age")
candidato.aic.3 <- c("mortality_rsi", "ahrq_ccs", "bmi", "month.8", "Age")
candidato.rfe.2 <- c("Age", "mortality_rsi", "bmi", "ahrq_ccs")
candidatos_4         <- list(candidato.aic, candidato.aic.2, candidato.aic.3, candidato.bic, candidato.bic.2, candidato.bic.3,
candidato.bic.4,candidato.rfe.lr, candidato.rfe.lr.2, candidato.rfe.rf, candidato.rfe.2)
nombres_candidatos_4 <- c("LOGISTICA AIC", "LOGISTICA AIC (sin 2 variables)" , "LOGISTICA AIC (TOP 5)" ,"LOGISTICA BIC", "LOGISTICA BIC (sin asa.status)" ,
"LOGISTICA BIC (TOP 5)", "LOGISTICA BIC (TOP 4)", "RFE LR TOP 18", "RFE LR TOP 3", "RFE RF TOP 5", "RFE RF TOP 4")
union4 <- cruzada_logistica(surgical_dataset, target, candidatos_4, nombres_candidatos_4,
grupos = 5, repe = 5)
load("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
sampsizes.2_aux <- list(200, 300, 400, 500)
bagging_modelo1_3_10folds_v2 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo1,
nodesizes = nodesizes.1,
sampsizes = sampsizes.2_aux, mtry = mtry.1,
ntree = n.trees.1, grupos = 10, repe = 20)
rm(sampsizes.2_aux)
nodesizes.2 <- list(20)
sampsizes.2 <- list(1, 500, 1000, 1500, 2000, 2500)
bagging_modelo2_3 <- tuneo_bagging(surgical_dataset, target = target,
lista.continua = var_modelo2,
nodesizes = nodesizes.2,
sampsizes = sampsizes.2, mtry = mtry.2,
ntree = n.trees.2, grupos = 5, repe = 10)
save.image("~/UCM/Machine Learning/Practica ML/MachineLearning/rdata/Bagging.RData")
